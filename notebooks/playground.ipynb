{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zarr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/calessi-iit.local/Projects/hannes-imitation')\n",
    "sys.path.append('/home/calessi-iit.local/Projects/hannes-imitation/hannes_imitation/external/diffusion_policy') # NOTE otherwise importing SequenceSampler fails\n",
    "\n",
    "# diffusion_policy imports\n",
    "from hannes_imitation.external.diffusion_policy.diffusion_policy.model.vision.multi_image_obs_encoder import MultiImageObsEncoder\n",
    "from hannes_imitation.external.diffusion_policy.diffusion_policy.model.vision.model_getter import get_resnet\n",
    "from hannes_imitation.external.diffusion_policy.diffusion_policy.policy.diffusion_unet_image_policy import DiffusionUnetImagePolicy\n",
    "from hannes_imitation.external.diffusion_policy.diffusion_policy.model.common.lr_scheduler import get_scheduler\n",
    "\n",
    "# hannes_imitation imports\n",
    "from hannes_imitation.dataset.hannes_dataset import HannesImageDataset\n",
    "from hannes_imitation.trainer.trainer_diffusion_policy import TrainerDiffusionPolicy\n",
    "from hannes_imitation.common import plot_utils\n",
    "\n",
    "# diffusers import\n",
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prova\n",
    "from hannes_imitation.dataset.hannes_dataset_hand_wrist_FE import HannesImageDatasetWrist\n",
    "\n",
    "merged_dir = '/home/calessi-iit.local/Projects/hannes-imitation/data/mustard_hand_wrist_FE/'\n",
    "merged_name = 'merged_hand_wrist_FE.zarr'\n",
    "zarr_path = os.path.join(merged_dir, merged_name)\n",
    "keys = ['image_in_hand', 'ref_move_hand', 'ref_move_wrist_FE']\n",
    "val_ratio = 0.1\n",
    "seed = 72\n",
    "max_train_episodes = None\n",
    "horizon = 16 # prediction horizon\n",
    "observation_horizon = 2\n",
    "action_horizon = 8\n",
    "pad_before = observation_horizon - 1\n",
    "pad_after = action_horizon - 1\n",
    "\n",
    "# training and validation dataset\n",
    "train_dataset = HannesImageDatasetWrist(zarr_path, keys, horizon=horizon, pad_before=pad_before, pad_after=pad_after, seed=seed, val_ratio=val_ratio, max_train_episodes=None)\n",
    "\n",
    "tr_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, num_workers=4, shuffle=True, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "batch = next(iter(tr_dataloader))\n",
    "\n",
    "batch['obs']['image_in_hand'].shape, batch['action'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dir = '/home/calessi-iit.local/Projects/hannes-imitation/data/preliminary/'\n",
    "merged_name = 'merged.zarr'\n",
    "zarr_path = os.path.join(merged_dir, merged_name)\n",
    "keys = ['image_in_hand', 'ref_move_hand']\n",
    "val_ratio = 0.1\n",
    "seed = 72\n",
    "max_train_episodes = None\n",
    "horizon = 16 # prediction horizon\n",
    "observation_horizon = 2\n",
    "action_horizon = 8\n",
    "pad_before = observation_horizon - 1\n",
    "pad_after = action_horizon - 1\n",
    "\n",
    "# training and validation dataset\n",
    "train_dataset = HannesImageDataset(zarr_path, keys, horizon=horizon, pad_before=pad_before, pad_after=pad_after, seed=seed, val_ratio=val_ratio, max_train_episodes=None)\n",
    "validation_dataset = train_dataset.get_validation_dataset()\n",
    "\n",
    "# get normalizer\n",
    "normalizer = train_dataset.get_normalizer()\n",
    "\n",
    "# create dataloaders for training and validation\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "shuffle = True\n",
    "\n",
    "# pin_memory = True accelerates cpu-gpu transfer\n",
    "# persistent_workers = True does not kill worker process after each epoch\n",
    "tr_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=True, persistent_workers=True)\n",
    "vl_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize data in batch\n",
    "# TODO discard unused observations earlier\n",
    "batch = next(iter(tr_dataloader))\n",
    "print(\"batch['obs']['image_hand']\", batch['obs']['image_in_hand'].shape, batch['obs']['image_in_hand'].dtype)\n",
    "print(\"batch['action']\", batch['action'].shape, batch['action'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get image observation encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  obs_encoder:\n",
    "    _target_: diffusion_policy.model.vision.multi_image_obs_encoder.MultiImageObsEncoder\n",
    "    shape_meta: ${shape_meta}\n",
    "    rgb_model:\n",
    "      _target_: diffusion_policy.model.vision.model_getter.get_resnet\n",
    "      name: resnet18\n",
    "      weights: null\n",
    "    resize_shape: null\n",
    "    crop_shape: [76, 76]\n",
    "    # constant center crop\n",
    "    random_crop: True\n",
    "    use_group_norm: True\n",
    "    share_rgb_model: False\n",
    "    imagenet_norm: True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image_shape: &image_shape [3, 96, 96]\n",
    "shape_meta: &shape_meta\n",
    "  # acceptable types: rgb, low_dim\n",
    "  obs:\n",
    "    image:\n",
    "      shape: *image_shape\n",
    "      type: rgb\n",
    "    agent_pos:\n",
    "      shape: [2]\n",
    "      type: low_dim\n",
    "  action:\n",
    "    shape: [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create shape_meta\n",
    "item = train_dataset.__getitem__(0)\n",
    "_, C, H, W = item['obs']['image_in_hand'].shape\n",
    "_, action_dim = item['action'].shape\n",
    "\n",
    "# Make sure that `shape_meta` correspond to input and output shapes for your task.\n",
    "shape_meta = dict(obs=dict(), action=dict())\n",
    "shape_meta['obs']['image_in_hand'] = dict(shape=(C, H, W), type='rgb')\n",
    "shape_meta['action'] = dict(shape=[action_dim])\n",
    "\n",
    "# NOTE if you want to include more observations\n",
    "#shape_meta['obs']['image_in_head'] = dict(shape=(C, H, W), type='rgb')\n",
    "#shape_meta['obs']['hand_info'] = dict(shape=[6], type='low_dim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create observation encoder\n",
    "rgb_model = get_resnet('resnet18') # dict()\n",
    "\n",
    "# The MultiImageObsEncoder encodes image and low dimensional observations into a single observation.\n",
    "# The constructor requires 2 positional arguments (shape_meta, rgb_model).\n",
    "# rgb_model can be directly an nn.Module or a Dict[str,nn.Module]\n",
    "\n",
    "# Optionally, you can specify if the image is resized (`resize_shape`) and/or cropped (`crop_shape`, `random_crop`) and/or\n",
    "# normalized according to imagenet values (`imagenet_norm`)\n",
    "# These transformations are performed in the forward() method.\n",
    "# We only use imagenet_norm.\n",
    "observation_encoder = MultiImageObsEncoder(shape_meta=shape_meta, rgb_model=rgb_model,\n",
    "                                       resize_shape=None,\n",
    "                                       crop_shape=None,\n",
    "                                       random_crop=False,\n",
    "                                       use_group_norm=True,\n",
    "                                       share_rgb_model=True,\n",
    "                                       imagenet_norm=True)\n",
    "\n",
    "# freeze observation_encoder\n",
    "_ = observation_encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#item['obs']['image_in_head'] = torch.zeros_like(item['obs']['image_in_hand'])\n",
    "#item['obs']['hand_info'] = torch.ones((16,6))\n",
    "\n",
    "# The forward model takes as input the dictinonary of observation `obs_dict`\n",
    "# if obs_dict contains more observations of type `rgb`, their features are concatenated\n",
    "# if obs_dict contains observations with low dimensions (of type `low_dim`), they are directly concatenated after the images' features\n",
    "observation_encoder(item['obs']).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from hannes_imitation.model.diffusion.conditional_unet1d import ConditionalUnet1D\n",
    "#from hannes_imitation.model.vision.model_getter import get_resnet\n",
    "\n",
    "\n",
    "#from hannes_imitation.common.pytorch_util import replace_bn_with_gn\n",
    "\n",
    "# diffusion policy imports\n",
    "#from hannes_imitation.external.diffusion_policy.diffusion_policy.model.diffusion.conditional_unet1d import ConditionalUnet1D\n",
    "#from hannes_imitation.external.diffusion_policy.diffusion_policy.model.vision.model_getter import get_resnet\n",
    "#from hannes_imitation.external.diffusion_policy.diffusion_policy.common.pytorch_util import replace_submodules\n",
    "\n",
    "#import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct ResNet18 encoder\n",
    "# if you have multiple camera views, use seperate encoder weights for each view.\n",
    "#vision_encoder = get_resnet('resnet18')\n",
    "\n",
    "# IMPORTANT!\n",
    "# replace all BatchNorm with GroupNorm to work with EMA\n",
    "# performance will tank if you forget to do this!\n",
    "#vision_encoder = replace_bn_with_gn(vision_encoder)\n",
    "\n",
    "#_ = vision_encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vision_feature_dim = observation_encoder.output_shape()[0] #512 #  ResNet18 output dimensionality\n",
    "#lowdim_obs_dim = 2 # # agent_pos is 2 dimensional\n",
    "\n",
    "#action_dim = batch['action'].shape[-1]\n",
    "#observation_dim = vision_feature_dim# + lowdim_obs_dim\n",
    "#conditioning_dim = observation_dim * observation_horizon\n",
    "\n",
    "# create network object\n",
    "# input_dim, local_cond_dim=None, global_cond_dim=None, diffusion_step_embed_dim=256, down_dims=[256,512,1024], kernel_size=3, n_groups=8, cond_predict_scale=False)\n",
    "#noise_predictor = ConditionalUnet1D(input_dim=action_dim, global_cond_dim=conditioning_dim, \n",
    "#                                   diffusion_step_embed_dim=256,\n",
    "#                                   down_dims=[32, 64, 128])#[256,512,1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the final arch has 2 parts\n",
    "#nets = nn.ModuleDict({'observation_encoder': observation_encoder, \n",
    "#                      'noise_predictor': noise_predictor})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device transfer\n",
    "#device = torch.device('cpu')\n",
    "#device = torch.device('cuda')\n",
    "#_ = nets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nets['observation_encoder'].training, nets['noise_predictor'].training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo 1\n",
    "#with torch.no_grad():\n",
    "#    # example inputs\n",
    "#    image = torch.zeros((1, observation_horizon, 3, 96, 96))\n",
    "#    #agent_pos = torch.zeros((1, obs_horizon, 2))\n",
    "#    \n",
    "#    # vision encoder\n",
    "#    image = image.flatten(end_dim=1) # squeeze, (obs_horizon=2, 3, 96, 96)\n",
    "#    image_features = nets['vision_encoder'](image) # (obs_horizon=2, 512)\n",
    "#    image_features = torch.unsqueeze(image_features, dim=0) # unsqueeze, (1,2,512)\n",
    "#    \n",
    "#    #obs = torch.cat([image_features, agent_pos], dim=-1) # (1,2,514)\n",
    "#    obs = image_features#\n",
    "#\n",
    "#    # action diffusion\n",
    "#    noised_action = torch.randn((1, prediction_horizon, action_dim))\n",
    "#    diffusion_iter = torch.zeros((1,))\n",
    "#\n",
    "#    # the noise prediction network\n",
    "#    # takes noisy action, diffusion iteration and observation as input\n",
    "#    # predicts the noise added to action (1, pred_horizon=16, action_dim=2)\n",
    "#    # the conditional observation gets flattened into (1, 1028)\n",
    "#    noise = nets['noise_predictor'](\n",
    "#        sample=noised_action,\n",
    "#        timestep=diffusion_iter,\n",
    "#        global_cond=obs.flatten(start_dim=1)) #\n",
    "#\n",
    "#    # illustration of removing noise\n",
    "#    # the actual noise removal is performed by NoiseScheduler\n",
    "#    # and is dependent on the diffusion noise schedule\n",
    "#    denoised_action = noised_action - noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create noise scheduler\n",
    "# for this demo, we use DDPMScheduler with 100 diffusion iterations\n",
    "# NOTE: the choice of beta schedule has big impact on performance. We found squared cosine works the best\n",
    "num_diffusion_iters = 50 #100 \n",
    "\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=num_diffusion_iters,\n",
    "                                beta_schedule='squaredcos_cap_v2',\n",
    "                                clip_sample=True, # clip output to [-1,1] to improve stability\n",
    "                                prediction_type='epsilon') # the network predicts noise (instead of denoised action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from hannes_imitation.model.diffusion.ema_model import EMAModel\n",
    "\n",
    "# Exponential Moving Average accelerates training and improves stability\n",
    "# holds a copy of the model weights\n",
    "#ema = EMAModel(parameters=nets.parameters(), power=0.75)\n",
    "#ema = EMAModel(model=nets, power=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resnet_normalizer = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#_ = normalizer.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DiffusionUnetImagePolicy requires 6 positional arguments\n",
    "# shape_meta is a dictionary that contains the shapes of observations and actions for the task\n",
    "# noise_scheduler is an instance of DDPMScheduler noise scheduler\n",
    "# obs_encoder is an instance of MultiImageObsEncoder which encodes images and low dimensional observations as conditioning\n",
    "# horizon is the prediction horizon (action prediction horizon)\n",
    "# n_action_steps is the action execution horizon (how many actions are actually executed from the prediction)\n",
    "# n_obs_steps is the observation horizon (how many recent observations to include as condition)\n",
    "\n",
    "# NOTE: there are other parameters that we do not change, except for the UNet model size\n",
    "policy = DiffusionUnetImagePolicy(shape_meta=shape_meta, \n",
    "                                  noise_scheduler=noise_scheduler, \n",
    "                                  obs_encoder=observation_encoder,\n",
    "                                  horizon=horizon,\n",
    "                                  n_action_steps=action_horizon,\n",
    "                                  n_obs_steps=observation_horizon,\n",
    "                                  diffusion_step_embed_dim=64, #128,#256 default,\n",
    "                                  down_dims=[16, 32, 64])#[32, 64, 128])#(256,512,1024)) default\n",
    "\n",
    "#_ = policy.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizer = normalizer.to(device)\n",
    "#policy.set_normalizer(normalizer=normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer and learning rate scheduler\n",
    "# Standard ADAM optimizer (NOTE that EMA parametesr are not optimized)\n",
    "optimizer = torch.optim.AdamW(params=policy.parameters(), lr=1e-4, weight_decay=1e-6)\n",
    "\n",
    "num_epochs = 200 #100 # 100\n",
    "\n",
    "# Cosine LR schedule with linear warmup\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='cosine',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=len(tr_dataloader) * num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train policy\n",
    "policy_trainer = TrainerDiffusionPolicy(policy=policy, \n",
    "                                        optimizer=optimizer, \n",
    "                                        normalizer=normalizer, \n",
    "                                        tr_dataloader=tr_dataloader, \n",
    "                                        vl_dataloader=vl_dataloader, \n",
    "                                        learning_rate_scheduler=lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = policy_trainer.run(num_epochs=num_epochs, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save policy and training results\n",
    "policy_dir = '/home/calessi-iit.local/Projects/hannes-imitation/trainings/'\n",
    "policy_name = 'preliminary_policy.pth'\n",
    "policy_path = os.path.join(policy_dir, policy_name)\n",
    "\n",
    "training_dict = {'policy': policy.to('cpu'),\n",
    "                 'policy_state_dict': policy.state_dict(),\n",
    "                 'optimizer': optimizer,\n",
    "                 'noise_scheduler': noise_scheduler,\n",
    "                 'history': history\n",
    "                }\n",
    "\n",
    "torch.save(training_dict, policy_path)\n",
    "\n",
    "print(\"Training saved in %s\" % str(policy_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test policy online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/calessi-iit.local/Projects/hannes-imitation/notebooks',\n",
       " '/home/calessi-iit.local/miniforge3/envs/robodiff/lib/python39.zip',\n",
       " '/home/calessi-iit.local/miniforge3/envs/robodiff/lib/python3.9',\n",
       " '/home/calessi-iit.local/miniforge3/envs/robodiff/lib/python3.9/lib-dynload',\n",
       " '',\n",
       " '/home/calessi-iit.local/miniforge3/envs/robodiff/lib/python3.9/site-packages',\n",
       " '/home/calessi-iit.local/Projects/hannes-imitation',\n",
       " '/home/calessi-iit.local/Projects/hannes-imitation/hannes_imitation/external/diffusion_policy',\n",
       " '/home/calessi-iit.local/miniforge3/envs/robodiff/lib/python3.9/site-packages/setuptools/_vendor']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import zarr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/calessi-iit.local/Projects/hannes-imitation')\n",
    "sys.path.append('/home/calessi-iit.local/Projects/hannes-imitation/hannes_imitation/external/diffusion_policy') # NOTE otherwise importing SequenceSampler fails\n",
    "\n",
    "# diffusion_policy imports\n",
    "from hannes_imitation.external.diffusion_policy.diffusion_policy.model.vision.multi_image_obs_encoder import MultiImageObsEncoder\n",
    "from hannes_imitation.external.diffusion_policy.diffusion_policy.model.vision.model_getter import get_resnet\n",
    "from hannes_imitation.external.diffusion_policy.diffusion_policy.policy.diffusion_unet_image_policy import DiffusionUnetImagePolicy\n",
    "from hannes_imitation.external.diffusion_policy.diffusion_policy.model.common.lr_scheduler import get_scheduler\n",
    "\n",
    "# hannes_imitation imports\n",
    "from hannes_imitation.common import plot_utils\n",
    "\n",
    "# diffusers import\n",
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import collections\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../hannes-imitation')\n",
    "sys.path.append('../../hannes-imitation/hannes_imitation/external/diffusion_policy') # NOTE otherwise importing SequenceSampler fails\n",
    "\n",
    "# diffusion_policy imports\n",
    "from hannes_imitation.external.diffusion_policy.diffusion_policy.policy.diffusion_unet_image_policy import DiffusionUnetImagePolicy\n",
    "from hannes_imitation.external.diffusion_policy.diffusion_policy.common.pytorch_util import dict_apply\n",
    "\n",
    "# hannes_imitation imports\n",
    "from hannes_imitation.dataset.hannes_dataset import HannesImageDataset\n",
    "from hannes_imitation.common import plot_utils\n",
    "from hannes_imitation.common.data_utils import resize_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hannes imports\n",
    "sys.path.append('../../pyHannesAPI/')\n",
    "\n",
    "from pyHannesAPI.pyHannes import Hannes\n",
    "from pyHannesAPI import pyHannes_commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing connection to Hannes\n",
      "Scanning...\n",
      "Found 1 device\n",
      "Connected to HANNESFA\n"
     ]
    }
   ],
   "source": [
    "hannes = Hannes(device_name='HANNESFA', timeout=None, read_size=1)\n",
    "\n",
    "hannes.connect()\n",
    "\n",
    "# In Command Mode is preferrable to set the flags for enabling/disabling streaming of measurements \n",
    "# (e.g. via the ReplyEMG, ReplyIMU, ReplySkin, ReplyMeasurements and ReplyVibro commands), \n",
    "# before enabling the stream by exiting the Command Mode.\n",
    "# NOTE: in questo modo evitiamo la sfilza di messaggi iniziali con header 32 e 33.\n",
    "# NOTE: comunque prima di un messaggio 19 riceviamo due messaggi (32, 33). \n",
    "hannes.enable_reply_measurements() # header 19\n",
    "#h.request_joints_limits() #  header 24 NOTE: ricevuti solo all'inizio\n",
    "#h.enable_reply_end_data_stream()\n",
    "#h.enable_reply_quaternions()  \n",
    "#h.enable_reply_gravity()\n",
    "#h.enable_reply_emg()\n",
    "\n",
    "hannes.set_control_modality(pyHannes_commands.HControlModality.CONTROL_UNITY)\n",
    "# now you can use hand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hannes.move_hand(0)\n",
    "hannes.move_wristFE(50)\n",
    "hannes.move_thumb_home()\n",
    "hannes.move_thumb_power()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open camera\n",
    "#cam = cv2.VideoCapture(4)\n",
    "\n",
    "#if not cam.isOpened():\n",
    "#    print(\"Cannot open camera\")\n",
    "\n",
    "#ret, image = cam.read()\n",
    "#cv2.imshow(\"frame\", image)\n",
    "#cv2.waitKey(5000)\n",
    "\n",
    "#cam.release()\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "policy_path = '/home/calessi-iit.local/Projects/hannes-imitation/trainings/preliminary_policy.pth'\n",
    "policy_path = '/home/calessi-iit.local/Projects/hannes-imitation/trainings/preliminary_policy_wrist_FE.pth'\n",
    "policy_path = '/home/calessi-iit.local/Projects/hannes-imitation/trainings/preliminary_policy_wrist_FE-tmp.pth'\n",
    "\n",
    "checkpoint = torch.load(policy_path)\n",
    "\n",
    "policy = checkpoint['policy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device transfer\n",
    "device = torch.device('cuda')\n",
    "_ = policy.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_horizon = 2\n",
    "action_horizon = 4\n",
    "episode_len = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from hannes_imitation.processes.frame_capture import HannesFrameCapture\n",
    "\n",
    "manager = multiprocessing.Manager()\n",
    "deque_lock = multiprocessing.Lock()\n",
    "flag_demo = manager.Event()\n",
    "flag_demo.clear()\n",
    "\n",
    "frames_list = manager.list() # NOTE: replaces collections.deque(iterable=[], maxlen=2) # (B=1, To, C, H, W)\n",
    "hand_list = manager.list()\n",
    "\n",
    "hannes_video_capture = HannesFrameCapture(camera_index=4, kwargs=dict(flag_demo=flag_demo, frames_list=frames_list, lock=deque_lock))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hannes_imitation.control.diffusion_policy import HannesDiffusionPolicyController\n",
    "import time\n",
    "\n",
    "controller = HannesDiffusionPolicyController(hannes=hannes, policy=policy, control_frequency=20, observation_horizon=observation_horizon, \n",
    "                                             action_horizon=action_horizon, frames_list=frames_list, hannes_states_list=hand_list, lock=deque_lock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyHannesAPI.data_dumper import HannesDataDumper\n",
    "from pyHannesAPI.pyHannes import timestamp\n",
    "\n",
    "# TODO: save frames_list, hand_list, action_trajectories, episode_duration\n",
    "test_data_dir = '/home/calessi-iit.local/Projects/hannes-imitation/data/test/'\n",
    "test_data_base_name = 'trial' + timestamp() + '.zarr'\n",
    "test_data_path = os.path.join(test_data_dir, test_data_base_name )\n",
    "\n",
    "store = zarr.open(test_data_path, mode='w')\n",
    "\n",
    "hannes_hand_capture = HannesDataDumper(kwargs={'hannes_state': hannes.hannes_state,\n",
    "                                       'flag_demo': flag_demo,\n",
    "                                       'hannes_state_cond': hannes.hannes_state_cond,\n",
    "                                       'hannes_state_list': hand_list,\n",
    "                                       'store': store})\n",
    "\n",
    "print(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Damping hand states...\n",
      "camera released\n"
     ]
    }
   ],
   "source": [
    "# TEST LOOP\n",
    "flag_demo.set()\n",
    "hannes_video_capture.start()\n",
    "hannes_hand_capture.start()\n",
    "\n",
    "action_trajectories = []\n",
    "action_timestamps = []\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(episode_len):\n",
    "\n",
    "    obs_dict = controller.get_observation() # NOTE potrebbe essere real_env.get_observation() simile a interaccia Gym \n",
    "    action_trajectory = controller.predict(obs_dict)\n",
    "    controller.step(action_trajectory)\n",
    "\n",
    "    action_trajectories.append(action_trajectory)\n",
    "    action_timestamps.append(time.time())\n",
    "\n",
    "    # visualize\n",
    "    #print(np.round(action_trajectory[0]))\n",
    "    #cv2.imshow(\"frame\", resize_image(frames_list[-1], scaling_factor=0.2))\n",
    "    #cv2.waitKey(1)\n",
    "    \n",
    "#cv2.destroyAllWindows()\n",
    "toc = time.time()\n",
    "\n",
    "flag_demo.clear()\n",
    "hannes_video_capture.join()\n",
    "hannes_hand_capture.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "hannes.move_hand(0)\n",
    "hannes.move_wristFE(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add more stuff to the store\n",
    "store['policy_actions'] = np.array(action_trajectories)\n",
    "store['prediction_timestamps'] = np.array(action_timestamps)\n",
    "store['test_duration'] = np.array([toc - tic])\n",
    "store['camera_frames'] = np.array(frames_list) # TODO: adapt video dumper?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### old (but shows how things work internally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(nets, vl_dataloader):\n",
    "    vl_batch_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for nbatch in vl_dataloader:\n",
    "            obs_dict = nbatch['obs']\n",
    "            \n",
    "            # discard unused observation\n",
    "            # flatten observation horizon and batch size into a single dimension (B*observation_horizon), keeping other dimensions\n",
    "            # send to device\n",
    "            obs_dict = dict_apply(obs_dict, lambda x: x[:,:observation_horizon,...])\n",
    "            obs_dict = dict_apply(obs_dict, lambda x: x.reshape(-1,*x.shape[2:])) # (B*observation_horizon, n_channels, H, W)\n",
    "            obs_dict = dict_apply(obs_dict, lambda x: x.to(device))\n",
    "\n",
    "            # normalize batch (discard unused observation)\n",
    "            #nimage = nbatch['obs']['image_in_hand'][:, :observation_horizon] # (B, observation_horizon, n_channels, H, W)\n",
    "            #nimage = nimage.to(device)\n",
    "            #nimage = resnet_normalizer(nimage)\n",
    "\n",
    "            naction = nbatch['action'].to(device)\n",
    "            naction = normalizer['action'].normalize(naction)\n",
    "            #B, _, n_channels, H, W = nimage.shape\n",
    "\n",
    "            B, _, _ = nbatch['action'].shape\n",
    "\n",
    "            # encoder vision features\n",
    "            #images = nimage.reshape(-1, n_channels, H, W) # (B*observation_horizon, n_channels, H, W)\n",
    "            #image_features = nets['vision_encoder'](images) # (B*observation_horizon, features_dim)\n",
    "            #image_features = image_features.reshape(B, observation_horizon, -1) # (B, observation_horizon, features_dim)\n",
    "\n",
    "            # conditional observation flattened. Concatenate vision feature and, if any, other low-dim obs\n",
    "            #obs_features = torch.cat([image_features, nagent_pos], dim=-1)\n",
    "            #obs_features = image_features\n",
    "            #obs_cond = obs_features.reshape(B, -1) # (B, obs_horizon * obs_dim)\n",
    "            obs_cond = observation_encoder(obs_dict).reshape(B, -1)\n",
    "\n",
    "            # sample noise to add to actions\n",
    "            noise = torch.randn(naction.shape, device=device)\n",
    "\n",
    "            # sample a diffusion iteration for each data point\n",
    "            timesteps = torch.randint(low=0, high=noise_scheduler.config.num_train_timesteps, size=(B,), device=device).long()\n",
    "\n",
    "            # Forward diffusion process. Add noise to the clean images according to the noise magnitude at each diffusion iteration\n",
    "            noisy_actions = noise_scheduler.add_noise(naction, noise, timesteps)\n",
    "\n",
    "            # predict the noise residual\n",
    "            # the noise prediction network takes noisy action, diffusion iteration and observation as input and predicts the noise added to naction (1, pred_horizon=16, action_dim=2)\n",
    "            noise_pred = nets['noise_predictor'](noisy_actions, timesteps, global_cond=obs_cond)\n",
    "\n",
    "            # L2 loss\n",
    "            loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "            vl_batch_losses.append(loss.item())\n",
    "    \n",
    "    vl_loss = np.mean(vl_batch_losses)\n",
    "\n",
    "    return vl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {'epoch': list(), \n",
    "           'tr_loss': list(), \n",
    "           'vl_loss': list()}\n",
    "\n",
    "with tqdm(range(num_epochs), desc='Epoch') as tglobal:\n",
    "    # epoch loop\n",
    "    for epoch_idx in tglobal:\n",
    "        epoch_loss = list()\n",
    "        # batch loop\n",
    "        with tqdm(tr_dataloader, desc='Batch', leave=False) as tepoch:\n",
    "            for nbatch in tepoch:\n",
    "                # normalize batch (discard unused observation)\n",
    "                #nimage = nbatch['obs']['image_in_hand'][:, :observation_horizon] # (B, observation_horizon, n_channels, H, W)\n",
    "                #nimage = nimage.to(device)\n",
    "                #nimage = resnet_normalizer(nimage)\n",
    "                \n",
    "                obs_dict = nbatch['obs']\n",
    "                \n",
    "                # discard unused observation\n",
    "                #for key in obs_dict.keys():\n",
    "                #    obs_dict[key] = obs_dict[key][:, :observation_horizon]\n",
    "                # discard unused observation\n",
    "                obs_dict = dict_apply(obs_dict, lambda x: x[:,:observation_horizon,...])\n",
    "                # flatten observation horizon and batch size into a single dimension (B*observation_horizon), keeping other dimensions\n",
    "                obs_dict = dict_apply(obs_dict, lambda x: x.reshape(-1,*x.shape[2:])) # (B*observation_horizon, n_channels, H, W)\n",
    "                # send to device\n",
    "                obs_dict = dict_apply(obs_dict, lambda x: x.to(device))\n",
    "\n",
    "                naction = nbatch['action'].to(device)\n",
    "                naction = normalizer['action'].normalize(naction)\n",
    "                #B, _, n_channels, H, W = nimage.shape\n",
    "                B, _, _ = nbatch['action'].shape\n",
    "\n",
    "                # encoder vision features\n",
    "                #images = nimage.reshape(-1, n_channels, H, W) # (B*observation_horizon, n_channels, H, W)\n",
    "                #image_features = nets['vision_encoder'](images) # (B*observation_horizon, features_dim)\n",
    "                #image_features = image_features.reshape(B, observation_horizon, -1) # (B, observation_horizon, features_dim)\n",
    "\n",
    "                # conditional observation flattened. Concatenate vision feature and, if any, other low-dim obs\n",
    "                #obs_features = torch.cat([image_features, nagent_pos], dim=-1)\n",
    "                #obs_features = image_features\n",
    "                #obs_cond = obs_features.reshape(B, -1) # (B, obs_horizon * obs_dim)\n",
    "\n",
    "                obs_cond = observation_encoder(obs_dict).reshape(B, -1)\n",
    "\n",
    "                # sample noise to add to actions\n",
    "                noise = torch.randn(naction.shape, device=device)\n",
    "\n",
    "                # sample a diffusion iteration for each data point\n",
    "                timesteps = torch.randint(low=0, high=noise_scheduler.config.num_train_timesteps, size=(B,), device=device).long()\n",
    "\n",
    "                # Forward diffusion process. Add noise to the clean images according to the noise magnitude at each diffusion iteration\n",
    "                noisy_actions = noise_scheduler.add_noise(naction, noise, timesteps)\n",
    "\n",
    "                # predict the noise residual\n",
    "                # the noise prediction network takes noisy action, diffusion iteration and observation as input and predicts the noise added to naction (1, pred_horizon=16, action_dim=2)\n",
    "                noise_pred = nets['noise_predictor'](noisy_actions, timesteps, global_cond=obs_cond)\n",
    "\n",
    "                # L2 loss\n",
    "                loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "                # optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                lr_scheduler.step() # step lr scheduler every batch. This is different from standard pytorch behavior\n",
    "\n",
    "                # update Exponential Moving Average of the model weights\n",
    "                #ema.step(nets.parameters())\n",
    "                #ema.step(nets)\n",
    "\n",
    "                # logging\n",
    "                loss_cpu = loss.item()\n",
    "                epoch_loss.append(loss_cpu)\n",
    "                tepoch.set_postfix(loss=loss_cpu)\n",
    "\n",
    "        # evaluate model on validation set after each epoch\n",
    "        vl_loss = validate(nets=nets.to(device), vl_dataloader=vl_dataloader)\n",
    "\n",
    "        tglobal.set_postfix(loss=np.mean(epoch_loss))\n",
    "\n",
    "        history['epoch'].append(epoch_idx + 1)\n",
    "        history['tr_loss'].append(np.mean(epoch_loss))\n",
    "        history['vl_loss'].append(vl_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robodiff",
   "language": "python",
   "name": "robodiff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
