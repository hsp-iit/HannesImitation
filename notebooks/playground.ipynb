{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/calessi-iit.local/Projects/hannes-imitation/notebooks',\n",
       " '/home/calessi-iit.local/miniforge3/envs/robodiff/lib/python39.zip',\n",
       " '/home/calessi-iit.local/miniforge3/envs/robodiff/lib/python3.9',\n",
       " '/home/calessi-iit.local/miniforge3/envs/robodiff/lib/python3.9/lib-dynload',\n",
       " '',\n",
       " '/home/calessi-iit.local/miniforge3/envs/robodiff/lib/python3.9/site-packages',\n",
       " '/home/calessi-iit.local/Projects/hannes-imitation',\n",
       " '/home/calessi-iit.local/Projects/hannes-imitation/hannes_imitation/external/diffusion_policy',\n",
       " '/home/calessi-iit.local/miniforge3/envs/robodiff/lib/python3.9/site-packages/setuptools/_vendor']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import zarr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/calessi-iit.local/Projects/hannes-imitation')\n",
    "sys.path.append('/home/calessi-iit.local/Projects/hannes-imitation/hannes_imitation/external/diffusion_policy') # NOTE otherwise importing SequenceSampler fails\n",
    "\n",
    "# diffusion_policy imports\n",
    "from hannes_imitation.external.diffusion_policy.diffusion_policy.model.vision.multi_image_obs_encoder import MultiImageObsEncoder\n",
    "from hannes_imitation.external.diffusion_policy.diffusion_policy.model.vision.model_getter import get_resnet\n",
    "from hannes_imitation.external.diffusion_policy.diffusion_policy.policy.diffusion_unet_image_policy import DiffusionUnetImagePolicy\n",
    "from hannes_imitation.external.diffusion_policy.diffusion_policy.model.common.lr_scheduler import get_scheduler\n",
    "\n",
    "# hannes_imitation imports\n",
    "from hannes_imitation.dataset.hannes_dataset import HannesImageDataset\n",
    "from hannes_imitation.trainer.trainer_diffusion_policy import TrainerDiffusionPolicy\n",
    "from hannes_imitation.common import plot_utils\n",
    "\n",
    "# diffusers import\n",
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cam = cv2.VideoCapture(4)\n",
    "\n",
    "while True:\n",
    "    _, frame = cam.read()\n",
    "    frame = cv2.rotate(frame, cv2.ROTATE_180)\n",
    "    cv2.imshow(\"window\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cam.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prova\n",
    "from hannes_imitation.dataset.hannes_dataset_hand_wrist_FE import HannesImageDatasetWrist\n",
    "\n",
    "merged_dir = '/home/calessi-iit.local/Projects/hannes-imitation/data/mustard_hand_wrist_FE/'\n",
    "merged_name = 'merged_hand_wrist_FE.zarr'\n",
    "zarr_path = os.path.join(merged_dir, merged_name)\n",
    "keys = ['image_in_hand', 'ref_move_hand', 'ref_move_wrist_FE']\n",
    "val_ratio = 0.1\n",
    "seed = 72\n",
    "max_train_episodes = None\n",
    "horizon = 16 # prediction horizon\n",
    "observation_horizon = 2\n",
    "action_horizon = 8\n",
    "pad_before = observation_horizon - 1\n",
    "pad_after = action_horizon - 1\n",
    "\n",
    "# training and validation dataset\n",
    "train_dataset = HannesImageDatasetWrist(zarr_path, keys, horizon=horizon, pad_before=pad_before, pad_after=pad_after, seed=seed, val_ratio=val_ratio, max_train_episodes=None)\n",
    "\n",
    "tr_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, num_workers=4, shuffle=True, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "batch = next(iter(tr_dataloader))\n",
    "\n",
    "batch['obs']['image_in_hand'].shape, batch['action'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dir = '/home/calessi-iit.local/Projects/hannes-imitation/data/preliminary/'\n",
    "merged_name = 'merged.zarr'\n",
    "zarr_path = os.path.join(merged_dir, merged_name)\n",
    "keys = ['image_in_hand', 'ref_move_hand']\n",
    "val_ratio = 0.1\n",
    "seed = 72\n",
    "max_train_episodes = None\n",
    "horizon = 16 # prediction horizon\n",
    "observation_horizon = 2\n",
    "action_horizon = 8\n",
    "pad_before = observation_horizon - 1\n",
    "pad_after = action_horizon - 1\n",
    "\n",
    "# training and validation dataset\n",
    "train_dataset = HannesImageDataset(zarr_path, keys, horizon=horizon, pad_before=pad_before, pad_after=pad_after, seed=seed, val_ratio=val_ratio, max_train_episodes=None)\n",
    "validation_dataset = train_dataset.get_validation_dataset()\n",
    "\n",
    "# get normalizer\n",
    "normalizer = train_dataset.get_normalizer()\n",
    "\n",
    "# create dataloaders for training and validation\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "shuffle = True\n",
    "\n",
    "# pin_memory = True accelerates cpu-gpu transfer\n",
    "# persistent_workers = True does not kill worker process after each epoch\n",
    "tr_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=True, persistent_workers=True)\n",
    "vl_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize data in batch\n",
    "# TODO discard unused observations earlier\n",
    "batch = next(iter(tr_dataloader))\n",
    "print(\"batch['obs']['image_hand']\", batch['obs']['image_in_hand'].shape, batch['obs']['image_in_hand'].dtype)\n",
    "print(\"batch['action']\", batch['action'].shape, batch['action'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get image observation encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  obs_encoder:\n",
    "    _target_: diffusion_policy.model.vision.multi_image_obs_encoder.MultiImageObsEncoder\n",
    "    shape_meta: ${shape_meta}\n",
    "    rgb_model:\n",
    "      _target_: diffusion_policy.model.vision.model_getter.get_resnet\n",
    "      name: resnet18\n",
    "      weights: null\n",
    "    resize_shape: null\n",
    "    crop_shape: [76, 76]\n",
    "    # constant center crop\n",
    "    random_crop: True\n",
    "    use_group_norm: True\n",
    "    share_rgb_model: False\n",
    "    imagenet_norm: True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image_shape: &image_shape [3, 96, 96]\n",
    "shape_meta: &shape_meta\n",
    "  # acceptable types: rgb, low_dim\n",
    "  obs:\n",
    "    image:\n",
    "      shape: *image_shape\n",
    "      type: rgb\n",
    "    agent_pos:\n",
    "      shape: [2]\n",
    "      type: low_dim\n",
    "  action:\n",
    "    shape: [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create shape_meta\n",
    "item = train_dataset.__getitem__(0)\n",
    "_, C, H, W = item['obs']['image_in_hand'].shape\n",
    "_, action_dim = item['action'].shape\n",
    "\n",
    "# Make sure that `shape_meta` correspond to input and output shapes for your task.\n",
    "shape_meta = dict(obs=dict(), action=dict())\n",
    "shape_meta['obs']['image_in_hand'] = dict(shape=(C, H, W), type='rgb')\n",
    "shape_meta['action'] = dict(shape=[action_dim])\n",
    "\n",
    "# NOTE if you want to include more observations\n",
    "#shape_meta['obs']['image_in_head'] = dict(shape=(C, H, W), type='rgb')\n",
    "#shape_meta['obs']['hand_info'] = dict(shape=[6], type='low_dim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create observation encoder\n",
    "rgb_model = get_resnet('resnet18') # dict()\n",
    "\n",
    "# The MultiImageObsEncoder encodes image and low dimensional observations into a single observation.\n",
    "# The constructor requires 2 positional arguments (shape_meta, rgb_model).\n",
    "# rgb_model can be directly an nn.Module or a Dict[str,nn.Module]\n",
    "\n",
    "# Optionally, you can specify if the image is resized (`resize_shape`) and/or cropped (`crop_shape`, `random_crop`) and/or\n",
    "# normalized according to imagenet values (`imagenet_norm`)\n",
    "# These transformations are performed in the forward() method.\n",
    "# We only use imagenet_norm.\n",
    "observation_encoder = MultiImageObsEncoder(shape_meta=shape_meta, rgb_model=rgb_model,\n",
    "                                       resize_shape=None,\n",
    "                                       crop_shape=None,\n",
    "                                       random_crop=False,\n",
    "                                       use_group_norm=True,\n",
    "                                       share_rgb_model=True,\n",
    "                                       imagenet_norm=True)\n",
    "\n",
    "# freeze observation_encoder\n",
    "_ = observation_encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#item['obs']['image_in_head'] = torch.zeros_like(item['obs']['image_in_hand'])\n",
    "#item['obs']['hand_info'] = torch.ones((16,6))\n",
    "\n",
    "# The forward model takes as input the dictinonary of observation `obs_dict`\n",
    "# if obs_dict contains more observations of type `rgb`, their features are concatenated\n",
    "# if obs_dict contains observations with low dimensions (of type `low_dim`), they are directly concatenated after the images' features\n",
    "observation_encoder(item['obs']).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from hannes_imitation.model.diffusion.conditional_unet1d import ConditionalUnet1D\n",
    "#from hannes_imitation.model.vision.model_getter import get_resnet\n",
    "\n",
    "\n",
    "#from hannes_imitation.common.pytorch_util import replace_bn_with_gn\n",
    "\n",
    "# diffusion policy imports\n",
    "#from hannes_imitation.external.diffusion_policy.diffusion_policy.model.diffusion.conditional_unet1d import ConditionalUnet1D\n",
    "#from hannes_imitation.external.diffusion_policy.diffusion_policy.model.vision.model_getter import get_resnet\n",
    "#from hannes_imitation.external.diffusion_policy.diffusion_policy.common.pytorch_util import replace_submodules\n",
    "\n",
    "#import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct ResNet18 encoder\n",
    "# if you have multiple camera views, use seperate encoder weights for each view.\n",
    "#vision_encoder = get_resnet('resnet18')\n",
    "\n",
    "# IMPORTANT!\n",
    "# replace all BatchNorm with GroupNorm to work with EMA\n",
    "# performance will tank if you forget to do this!\n",
    "#vision_encoder = replace_bn_with_gn(vision_encoder)\n",
    "\n",
    "#_ = vision_encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vision_feature_dim = observation_encoder.output_shape()[0] #512 #  ResNet18 output dimensionality\n",
    "#lowdim_obs_dim = 2 # # agent_pos is 2 dimensional\n",
    "\n",
    "#action_dim = batch['action'].shape[-1]\n",
    "#observation_dim = vision_feature_dim# + lowdim_obs_dim\n",
    "#conditioning_dim = observation_dim * observation_horizon\n",
    "\n",
    "# create network object\n",
    "# input_dim, local_cond_dim=None, global_cond_dim=None, diffusion_step_embed_dim=256, down_dims=[256,512,1024], kernel_size=3, n_groups=8, cond_predict_scale=False)\n",
    "#noise_predictor = ConditionalUnet1D(input_dim=action_dim, global_cond_dim=conditioning_dim, \n",
    "#                                   diffusion_step_embed_dim=256,\n",
    "#                                   down_dims=[32, 64, 128])#[256,512,1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the final arch has 2 parts\n",
    "#nets = nn.ModuleDict({'observation_encoder': observation_encoder, \n",
    "#                      'noise_predictor': noise_predictor})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device transfer\n",
    "#device = torch.device('cpu')\n",
    "#device = torch.device('cuda')\n",
    "#_ = nets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nets['observation_encoder'].training, nets['noise_predictor'].training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo 1\n",
    "#with torch.no_grad():\n",
    "#    # example inputs\n",
    "#    image = torch.zeros((1, observation_horizon, 3, 96, 96))\n",
    "#    #agent_pos = torch.zeros((1, obs_horizon, 2))\n",
    "#    \n",
    "#    # vision encoder\n",
    "#    image = image.flatten(end_dim=1) # squeeze, (obs_horizon=2, 3, 96, 96)\n",
    "#    image_features = nets['vision_encoder'](image) # (obs_horizon=2, 512)\n",
    "#    image_features = torch.unsqueeze(image_features, dim=0) # unsqueeze, (1,2,512)\n",
    "#    \n",
    "#    #obs = torch.cat([image_features, agent_pos], dim=-1) # (1,2,514)\n",
    "#    obs = image_features#\n",
    "#\n",
    "#    # action diffusion\n",
    "#    noised_action = torch.randn((1, prediction_horizon, action_dim))\n",
    "#    diffusion_iter = torch.zeros((1,))\n",
    "#\n",
    "#    # the noise prediction network\n",
    "#    # takes noisy action, diffusion iteration and observation as input\n",
    "#    # predicts the noise added to action (1, pred_horizon=16, action_dim=2)\n",
    "#    # the conditional observation gets flattened into (1, 1028)\n",
    "#    noise = nets['noise_predictor'](\n",
    "#        sample=noised_action,\n",
    "#        timestep=diffusion_iter,\n",
    "#        global_cond=obs.flatten(start_dim=1)) #\n",
    "#\n",
    "#    # illustration of removing noise\n",
    "#    # the actual noise removal is performed by NoiseScheduler\n",
    "#    # and is dependent on the diffusion noise schedule\n",
    "#    denoised_action = noised_action - noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create noise scheduler\n",
    "# for this demo, we use DDPMScheduler with 100 diffusion iterations\n",
    "# NOTE: the choice of beta schedule has big impact on performance. We found squared cosine works the best\n",
    "num_diffusion_iters = 50 #100 \n",
    "\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=num_diffusion_iters,\n",
    "                                beta_schedule='squaredcos_cap_v2',\n",
    "                                clip_sample=True, # clip output to [-1,1] to improve stability\n",
    "                                prediction_type='epsilon') # the network predicts noise (instead of denoised action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from hannes_imitation.model.diffusion.ema_model import EMAModel\n",
    "\n",
    "# Exponential Moving Average accelerates training and improves stability\n",
    "# holds a copy of the model weights\n",
    "#ema = EMAModel(parameters=nets.parameters(), power=0.75)\n",
    "#ema = EMAModel(model=nets, power=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resnet_normalizer = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#_ = normalizer.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DiffusionUnetImagePolicy requires 6 positional arguments\n",
    "# shape_meta is a dictionary that contains the shapes of observations and actions for the task\n",
    "# noise_scheduler is an instance of DDPMScheduler noise scheduler\n",
    "# obs_encoder is an instance of MultiImageObsEncoder which encodes images and low dimensional observations as conditioning\n",
    "# horizon is the prediction horizon (action prediction horizon)\n",
    "# n_action_steps is the action execution horizon (how many actions are actually executed from the prediction)\n",
    "# n_obs_steps is the observation horizon (how many recent observations to include as condition)\n",
    "\n",
    "# NOTE: there are other parameters that we do not change, except for the UNet model size\n",
    "policy = DiffusionUnetImagePolicy(shape_meta=shape_meta, \n",
    "                                  noise_scheduler=noise_scheduler, \n",
    "                                  obs_encoder=observation_encoder,\n",
    "                                  horizon=horizon,\n",
    "                                  n_action_steps=action_horizon,\n",
    "                                  n_obs_steps=observation_horizon,\n",
    "                                  diffusion_step_embed_dim=64, #128,#256 default,\n",
    "                                  down_dims=[16, 32, 64])#[32, 64, 128])#(256,512,1024)) default\n",
    "\n",
    "#_ = policy.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizer = normalizer.to(device)\n",
    "#policy.set_normalizer(normalizer=normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer and learning rate scheduler\n",
    "# Standard ADAM optimizer (NOTE that EMA parametesr are not optimized)\n",
    "optimizer = torch.optim.AdamW(params=policy.parameters(), lr=1e-4, weight_decay=1e-6)\n",
    "\n",
    "num_epochs = 200 #100 # 100\n",
    "\n",
    "# Cosine LR schedule with linear warmup\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='cosine',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=len(tr_dataloader) * num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train policy\n",
    "policy_trainer = TrainerDiffusionPolicy(policy=policy, \n",
    "                                        optimizer=optimizer, \n",
    "                                        normalizer=normalizer, \n",
    "                                        tr_dataloader=tr_dataloader, \n",
    "                                        vl_dataloader=vl_dataloader, \n",
    "                                        learning_rate_scheduler=lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = policy_trainer.run(num_epochs=num_epochs, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save policy and training results\n",
    "policy_dir = '/home/calessi-iit.local/Projects/hannes-imitation/trainings/'\n",
    "policy_name = 'preliminary_policy.pth'\n",
    "policy_path = os.path.join(policy_dir, policy_name)\n",
    "\n",
    "training_dict = {'policy': policy.to('cpu'),\n",
    "                 'policy_state_dict': policy.state_dict(),\n",
    "                 'optimizer': optimizer,\n",
    "                 'noise_scheduler': noise_scheduler,\n",
    "                 'history': history\n",
    "                }\n",
    "\n",
    "torch.save(training_dict, policy_path)\n",
    "\n",
    "print(\"Training saved in %s\" % str(policy_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [OLD - moved in main.py] Test policy online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/calessi-iit.local/Projects/hannes-imitation/notebooks',\n",
       " '/home/calessi-iit.local/miniforge3/envs/robodiff/lib/python39.zip',\n",
       " '/home/calessi-iit.local/miniforge3/envs/robodiff/lib/python3.9',\n",
       " '/home/calessi-iit.local/miniforge3/envs/robodiff/lib/python3.9/lib-dynload',\n",
       " '',\n",
       " '/home/calessi-iit.local/miniforge3/envs/robodiff/lib/python3.9/site-packages',\n",
       " '/home/calessi-iit.local/Projects/hannes-imitation',\n",
       " '/home/calessi-iit.local/Projects/hannes-imitation/hannes_imitation/external/diffusion_policy',\n",
       " '/home/calessi-iit.local/miniforge3/envs/robodiff/lib/python3.9/site-packages/setuptools/_vendor']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import zarr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/calessi-iit.local/Projects/hannes-imitation')\n",
    "sys.path.append('/home/calessi-iit.local/Projects/hannes-imitation/hannes_imitation/external/diffusion_policy') # NOTE otherwise importing SequenceSampler fails\n",
    "\n",
    "# diffusion_policy imports\n",
    "from hannes_imitation.external.diffusion_policy.diffusion_policy.model.vision.multi_image_obs_encoder import MultiImageObsEncoder\n",
    "from hannes_imitation.external.diffusion_policy.diffusion_policy.model.vision.model_getter import get_resnet\n",
    "from hannes_imitation.external.diffusion_policy.diffusion_policy.policy.diffusion_unet_image_policy import DiffusionUnetImagePolicy\n",
    "from hannes_imitation.external.diffusion_policy.diffusion_policy.model.common.lr_scheduler import get_scheduler\n",
    "\n",
    "# hannes_imitation imports\n",
    "from hannes_imitation.common import plot_utils\n",
    "\n",
    "# diffusers import\n",
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import collections\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../hannes-imitation')\n",
    "sys.path.append('../../hannes-imitation/hannes_imitation/external/diffusion_policy') # NOTE otherwise importing SequenceSampler fails\n",
    "\n",
    "# diffusion_policy imports\n",
    "from hannes_imitation.external.diffusion_policy.diffusion_policy.policy.diffusion_unet_image_policy import DiffusionUnetImagePolicy\n",
    "from hannes_imitation.external.diffusion_policy.diffusion_policy.common.pytorch_util import dict_apply\n",
    "\n",
    "# hannes_imitation imports\n",
    "from hannes_imitation.dataset.hannes_dataset import HannesImageDataset\n",
    "from hannes_imitation.common import plot_utils\n",
    "from hannes_imitation.common.data_utils import resize_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hannes imports\n",
    "sys.path.append('../../pyHannesAPI/')\n",
    "\n",
    "from pyHannesAPI.pyHannes import Hannes\n",
    "from pyHannesAPI import pyHannes_commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hannes = Hannes(device_name='HANNESFA', timeout=None, read_size=1)\n",
    "\n",
    "hannes.connect()\n",
    "\n",
    "# In Command Mode is preferrable to set the flags for enabling/disabling streaming of measurements \n",
    "# (e.g. via the ReplyEMG, ReplyIMU, ReplySkin, ReplyMeasurements and ReplyVibro commands), \n",
    "# before enabling the stream by exiting the Command Mode.\n",
    "# NOTE: in questo modo evitiamo la sfilza di messaggi iniziali con header 32 e 33.\n",
    "# NOTE: comunque prima di un messaggio 19 riceviamo due messaggi (32, 33). \n",
    "hannes.enable_reply_measurements() # header 19\n",
    "#h.request_joints_limits() #  header 24 NOTE: ricevuti solo all'inizio\n",
    "#h.enable_reply_end_data_stream()\n",
    "#h.enable_reply_quaternions()  \n",
    "#h.enable_reply_gravity()\n",
    "#h.enable_reply_emg()\n",
    "\n",
    "hannes.set_control_modality(pyHannes_commands.HControlModality.CONTROL_UNITY)\n",
    "# now you can use hand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hannes.move_wristFE(50+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hannes.move_wristPS(30)\n",
    "#time.sleep(2)\n",
    "#hannes.move_wristPS(-30)\n",
    "#time.sleep(2)\n",
    "#hannes.move_wristPS(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move_hand_close = np.random.randint(low=2, high=10+1, size=10)\n",
    "\n",
    "#plt.plot(np.cumsum(move_hand_close))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hannes.move_hand(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hannes.move_hand(0)\n",
    "#hannes.move_wristFE(50)\n",
    "#hannes.move_thumb_home()\n",
    "#hannes.move_thumb_power()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hannes.move_hand(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open camera\n",
    "#cam = cv2.VideoCapture(4)\n",
    "\n",
    "#if not cam.isOpened():\n",
    "#    print(\"Cannot open camera\")\n",
    "\n",
    "#ret, image = cam.read()\n",
    "#cv2.imshow(\"frame\", image)\n",
    "#cv2.waitKey(5000)\n",
    "\n",
    "#cam.release()\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "policy_path = '/home/calessi-iit.local/Projects/hannes-imitation/trainings/preliminary_policy.pth'\n",
    "policy_path = '/home/calessi-iit.local/Projects/hannes-imitation/trainings/preliminary_policy_wrist_FE.pth'\n",
    "policy_path = '/home/calessi-iit.local/Projects/hannes-imitation/trainings/preliminary_policy_wrist_FE-tmp.pth'\n",
    "\n",
    "checkpoint = torch.load(policy_path)\n",
    "\n",
    "policy = checkpoint['policy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device transfer\n",
    "device = torch.device('cuda')\n",
    "_ = policy.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_horizon = policy.n_obs_steps\n",
    "action_horizon = policy.n_action_steps\n",
    "episode_len = 30\n",
    "\n",
    "print(\"Observation horizon: %d\" % observation_horizon)\n",
    "print(\"Action horizon: %d\" % action_horizon)\n",
    "print(\"Diffusion iterations: %d\" % policy.num_inference_steps)\n",
    "print(\"Episode duration (n. policy calls): %d\" % episode_len)\n",
    "print(\"Device: %s\" %  device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from hannes_imitation.processes.frame_capture import HannesFrameCapture\n",
    "from pyHannesAPI.video_dumper import HannesAsynchronousVideoDumper\n",
    "from pyHannesAPI.camera.opencv_camera import OpenCVCamera\n",
    "\n",
    "manager = multiprocessing.Manager()\n",
    "deque_lock = multiprocessing.Lock()\n",
    "flag_demo = manager.Event()\n",
    "flag_demo.clear()\n",
    "\n",
    "frames_list = manager.list() # NOTE: replaces collections.deque(iterable=[], maxlen=2) # (B=1, To, C, H, W)\n",
    "frames_timestamps_list = manager.list()\n",
    "hand_list = manager.list()\n",
    "\n",
    "#hannes_video_capture = HannesFrameCapture(camera_index=4, kwargs=dict(flag_demo=flag_demo, frames_list=frames_list, lock=deque_lock))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_hand_camera = OpenCVCamera(camera_index=4, width=640, height=480)\n",
    "# set camera properties\n",
    "in_hand_camera.camera.set(cv2.CAP_PROP_AUTO_EXPOSURE, 1) # default 3, 1 disables autoexposure\n",
    "in_hand_camera.camera.set(cv2.CAP_PROP_EXPOSURE, 500) # default 166. 300=30fps, 400=25fps, 500=20fps\n",
    "in_hand_camera.camera.set(cv2.CAP_PROP_GAIN, 100) # 0-128, default 64 (gain=100 is ok when auto exposure is disabled and exposure is 500)\n",
    "assert(in_hand_camera.camera.get(cv2.CAP_PROP_AUTO_EXPOSURE) == 1)\n",
    "assert(in_hand_camera.camera.get(cv2.CAP_PROP_EXPOSURE) == 500)\n",
    "assert(in_hand_camera.camera.get(cv2.CAP_PROP_GAIN) == 100)\n",
    "\n",
    "hannes_video_capture = HannesAsynchronousVideoDumper(camera=in_hand_camera, kwargs=dict(flag_demo=flag_demo, frames_list=frames_list, timestamps_list=frames_timestamps_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_demo.set()\n",
    "hannes_video_capture.start()\n",
    "time.sleep(3)\n",
    "\n",
    "flag_demo.clear()\n",
    "hannes_video_capture.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rgb': array([[[ 98, 102,  90],\n",
       "         [100, 104,  92],\n",
       "         [ 99, 102,  93],\n",
       "         ...,\n",
       "         [179, 189, 172],\n",
       "         [177, 194, 171],\n",
       "         [179, 196, 173]],\n",
       " \n",
       "        [[ 97,  99,  87],\n",
       "         [ 99, 100,  89],\n",
       "         [ 99, 100,  89],\n",
       "         ...,\n",
       "         [173, 186, 168],\n",
       "         [176, 188, 170],\n",
       "         [178, 191, 173]],\n",
       " \n",
       "        [[ 96,  98,  86],\n",
       "         [ 97,  99,  87],\n",
       "         [ 97,  99,  87],\n",
       "         ...,\n",
       "         [173, 185, 170],\n",
       "         [174, 186, 171],\n",
       "         [172, 184, 169]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[101, 112, 101],\n",
       "         [105, 115, 104],\n",
       "         [105, 115, 104],\n",
       "         ...,\n",
       "         [126, 149, 141],\n",
       "         [119, 148, 150],\n",
       "         [123, 152, 154]],\n",
       " \n",
       "        [[105, 115, 104],\n",
       "         [105, 115, 104],\n",
       "         [103, 114, 103],\n",
       "         ...,\n",
       "         [126, 149, 141],\n",
       "         [122, 150, 153],\n",
       "         [123, 152, 154]],\n",
       " \n",
       "        [[106, 119, 108],\n",
       "         [105, 118, 107],\n",
       "         [103, 115, 104],\n",
       "         ...,\n",
       "         [130, 151, 148],\n",
       "         [123, 151, 155],\n",
       "         [126, 154, 159]]], dtype=uint8)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hannes_imitation.control.diffusion_policy import HannesDiffusionPolicyController\n",
    "import time\n",
    "\n",
    "controller = HannesDiffusionPolicyController(hannes=hannes, policy=policy, control_frequency=20, observation_horizon=observation_horizon, \n",
    "                                             action_horizon=action_horizon, frames_list=frames_list, hannes_states_list=hand_list, lock=deque_lock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyHannesAPI.data_dumper import HannesDataDumper\n",
    "from pyHannesAPI.pyHannes import timestamp\n",
    "\n",
    "# TODO: save frames_list, hand_list, action_trajectories, episode_duration\n",
    "test_data_dir = '/home/calessi-iit.local/Projects/hannes-imitation/data/test/'\n",
    "test_data_base_name = 'trial' + timestamp() + '.zarr'\n",
    "test_data_path = os.path.join(test_data_dir, test_data_base_name )\n",
    "\n",
    "store = zarr.open(test_data_path, mode='w')\n",
    "\n",
    "hannes_hand_capture = HannesDataDumper(kwargs={'hannes_state': hannes.hannes_state,\n",
    "                                       'flag_demo': flag_demo,\n",
    "                                       'hannes_state_cond': hannes.hannes_state_cond,\n",
    "                                       'hannes_state_list': hand_list,\n",
    "                                       'store': store})\n",
    "\n",
    "print(\"Store path: %s\" test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST LOOP\n",
    "flag_demo.set()\n",
    "hannes_video_capture.start()\n",
    "hannes_hand_capture.start()\n",
    "\n",
    "action_trajectories = []\n",
    "action_timestamps = []\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(episode_len):\n",
    "\n",
    "    obs_dict = controller.get_observation() # NOTE potrebbe essere real_env.get_observation() simile a interaccia Gym \n",
    "    action_trajectory = controller.predict(obs_dict)\n",
    "    controller.step(action_trajectory)\n",
    "\n",
    "    action_trajectories.append(action_trajectory)\n",
    "    action_timestamps.append(time.time())\n",
    "\n",
    "    # visualize\n",
    "    #print(np.round(action_trajectory[0]))\n",
    "    #cv2.imshow(\"frame\", resize_image(frames_list[-1], scaling_factor=0.2))\n",
    "    #cv2.waitKey(1)\n",
    "    \n",
    "#cv2.destroyAllWindows()\n",
    "toc = time.time()\n",
    "\n",
    "flag_demo.clear()\n",
    "hannes_video_capture.join()\n",
    "hannes_hand_capture.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset robot\n",
    "hannes.move_hand(0)\n",
    "hannes.move_wristFE(50)\n",
    "hannes.move_wristPS(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add more stuff to the store\n",
    "store['policy_actions'] = np.array(action_trajectories)\n",
    "store['prediction_timestamps'] = np.array(action_timestamps)\n",
    "store['test_duration'] = np.array([toc - tic])\n",
    "store['camera_frames'] = np.array(frames_list) \n",
    "store['camera_timestamps'] = np.array(frames_timestamps_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### old (but shows how things work internally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(nets, vl_dataloader):\n",
    "    vl_batch_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for nbatch in vl_dataloader:\n",
    "            obs_dict = nbatch['obs']\n",
    "            \n",
    "            # discard unused observation\n",
    "            # flatten observation horizon and batch size into a single dimension (B*observation_horizon), keeping other dimensions\n",
    "            # send to device\n",
    "            obs_dict = dict_apply(obs_dict, lambda x: x[:,:observation_horizon,...])\n",
    "            obs_dict = dict_apply(obs_dict, lambda x: x.reshape(-1,*x.shape[2:])) # (B*observation_horizon, n_channels, H, W)\n",
    "            obs_dict = dict_apply(obs_dict, lambda x: x.to(device))\n",
    "\n",
    "            # normalize batch (discard unused observation)\n",
    "            #nimage = nbatch['obs']['image_in_hand'][:, :observation_horizon] # (B, observation_horizon, n_channels, H, W)\n",
    "            #nimage = nimage.to(device)\n",
    "            #nimage = resnet_normalizer(nimage)\n",
    "\n",
    "            naction = nbatch['action'].to(device)\n",
    "            naction = normalizer['action'].normalize(naction)\n",
    "            #B, _, n_channels, H, W = nimage.shape\n",
    "\n",
    "            B, _, _ = nbatch['action'].shape\n",
    "\n",
    "            # encoder vision features\n",
    "            #images = nimage.reshape(-1, n_channels, H, W) # (B*observation_horizon, n_channels, H, W)\n",
    "            #image_features = nets['vision_encoder'](images) # (B*observation_horizon, features_dim)\n",
    "            #image_features = image_features.reshape(B, observation_horizon, -1) # (B, observation_horizon, features_dim)\n",
    "\n",
    "            # conditional observation flattened. Concatenate vision feature and, if any, other low-dim obs\n",
    "            #obs_features = torch.cat([image_features, nagent_pos], dim=-1)\n",
    "            #obs_features = image_features\n",
    "            #obs_cond = obs_features.reshape(B, -1) # (B, obs_horizon * obs_dim)\n",
    "            obs_cond = observation_encoder(obs_dict).reshape(B, -1)\n",
    "\n",
    "            # sample noise to add to actions\n",
    "            noise = torch.randn(naction.shape, device=device)\n",
    "\n",
    "            # sample a diffusion iteration for each data point\n",
    "            timesteps = torch.randint(low=0, high=noise_scheduler.config.num_train_timesteps, size=(B,), device=device).long()\n",
    "\n",
    "            # Forward diffusion process. Add noise to the clean images according to the noise magnitude at each diffusion iteration\n",
    "            noisy_actions = noise_scheduler.add_noise(naction, noise, timesteps)\n",
    "\n",
    "            # predict the noise residual\n",
    "            # the noise prediction network takes noisy action, diffusion iteration and observation as input and predicts the noise added to naction (1, pred_horizon=16, action_dim=2)\n",
    "            noise_pred = nets['noise_predictor'](noisy_actions, timesteps, global_cond=obs_cond)\n",
    "\n",
    "            # L2 loss\n",
    "            loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "            vl_batch_losses.append(loss.item())\n",
    "    \n",
    "    vl_loss = np.mean(vl_batch_losses)\n",
    "\n",
    "    return vl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {'epoch': list(), \n",
    "           'tr_loss': list(), \n",
    "           'vl_loss': list()}\n",
    "\n",
    "with tqdm(range(num_epochs), desc='Epoch') as tglobal:\n",
    "    # epoch loop\n",
    "    for epoch_idx in tglobal:\n",
    "        epoch_loss = list()\n",
    "        # batch loop\n",
    "        with tqdm(tr_dataloader, desc='Batch', leave=False) as tepoch:\n",
    "            for nbatch in tepoch:\n",
    "                # normalize batch (discard unused observation)\n",
    "                #nimage = nbatch['obs']['image_in_hand'][:, :observation_horizon] # (B, observation_horizon, n_channels, H, W)\n",
    "                #nimage = nimage.to(device)\n",
    "                #nimage = resnet_normalizer(nimage)\n",
    "                \n",
    "                obs_dict = nbatch['obs']\n",
    "                \n",
    "                # discard unused observation\n",
    "                #for key in obs_dict.keys():\n",
    "                #    obs_dict[key] = obs_dict[key][:, :observation_horizon]\n",
    "                # discard unused observation\n",
    "                obs_dict = dict_apply(obs_dict, lambda x: x[:,:observation_horizon,...])\n",
    "                # flatten observation horizon and batch size into a single dimension (B*observation_horizon), keeping other dimensions\n",
    "                obs_dict = dict_apply(obs_dict, lambda x: x.reshape(-1,*x.shape[2:])) # (B*observation_horizon, n_channels, H, W)\n",
    "                # send to device\n",
    "                obs_dict = dict_apply(obs_dict, lambda x: x.to(device))\n",
    "\n",
    "                naction = nbatch['action'].to(device)\n",
    "                naction = normalizer['action'].normalize(naction)\n",
    "                #B, _, n_channels, H, W = nimage.shape\n",
    "                B, _, _ = nbatch['action'].shape\n",
    "\n",
    "                # encoder vision features\n",
    "                #images = nimage.reshape(-1, n_channels, H, W) # (B*observation_horizon, n_channels, H, W)\n",
    "                #image_features = nets['vision_encoder'](images) # (B*observation_horizon, features_dim)\n",
    "                #image_features = image_features.reshape(B, observation_horizon, -1) # (B, observation_horizon, features_dim)\n",
    "\n",
    "                # conditional observation flattened. Concatenate vision feature and, if any, other low-dim obs\n",
    "                #obs_features = torch.cat([image_features, nagent_pos], dim=-1)\n",
    "                #obs_features = image_features\n",
    "                #obs_cond = obs_features.reshape(B, -1) # (B, obs_horizon * obs_dim)\n",
    "\n",
    "                obs_cond = observation_encoder(obs_dict).reshape(B, -1)\n",
    "\n",
    "                # sample noise to add to actions\n",
    "                noise = torch.randn(naction.shape, device=device)\n",
    "\n",
    "                # sample a diffusion iteration for each data point\n",
    "                timesteps = torch.randint(low=0, high=noise_scheduler.config.num_train_timesteps, size=(B,), device=device).long()\n",
    "\n",
    "                # Forward diffusion process. Add noise to the clean images according to the noise magnitude at each diffusion iteration\n",
    "                noisy_actions = noise_scheduler.add_noise(naction, noise, timesteps)\n",
    "\n",
    "                # predict the noise residual\n",
    "                # the noise prediction network takes noisy action, diffusion iteration and observation as input and predicts the noise added to naction (1, pred_horizon=16, action_dim=2)\n",
    "                noise_pred = nets['noise_predictor'](noisy_actions, timesteps, global_cond=obs_cond)\n",
    "\n",
    "                # L2 loss\n",
    "                loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "                # optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                lr_scheduler.step() # step lr scheduler every batch. This is different from standard pytorch behavior\n",
    "\n",
    "                # update Exponential Moving Average of the model weights\n",
    "                #ema.step(nets.parameters())\n",
    "                #ema.step(nets)\n",
    "\n",
    "                # logging\n",
    "                loss_cpu = loss.item()\n",
    "                epoch_loss.append(loss_cpu)\n",
    "                tepoch.set_postfix(loss=loss_cpu)\n",
    "\n",
    "        # evaluate model on validation set after each epoch\n",
    "        vl_loss = validate(nets=nets.to(device), vl_dataloader=vl_dataloader)\n",
    "\n",
    "        tglobal.set_postfix(loss=np.mean(epoch_loss))\n",
    "\n",
    "        history['epoch'].append(epoch_idx + 1)\n",
    "        history['tr_loss'].append(np.mean(epoch_loss))\n",
    "        history['vl_loss'].append(vl_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check diffusers (noise schedulers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import PNDMScheduler, DDPMScheduler\n",
    "from diffusers import DPMSolverSinglestepScheduler, DDIMScheduler\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "#noise_scheduler = DPMSolverSinglestepScheduler(num_train_timesteps=num_diffusion_iters,  \n",
    "#                                beta_schedule=\"squaredcos_cap_v2\",\n",
    "#                                #clip_sample=True,\n",
    "#                                prediction_type=\"epsilon\")  \n",
    "\n",
    "ddim_noise_scheduler = DDIMScheduler(num_train_timesteps=100,\n",
    "                                beta_schedule='squaredcos_cap_v2',\n",
    "                                clip_sample=True, # clip output to [-1,1] to improve stability\n",
    "                                prediction_type='epsilon') # the network predicts noise (instead of denoised action)\n",
    "\n",
    "# NOTE default\n",
    "ddpm_noise_scheduler = DDPMScheduler(num_train_timesteps=100,\n",
    "                                beta_schedule='squaredcos_cap_v2', # squaredcos_cap_v2\n",
    "                                clip_sample=True, # clip output to [-1,1] to improve stability\n",
    "                                prediction_type='epsilon') # the network predicts noise (instead of denoised action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x78a8db5efcd0>]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ddpm_noise_scheduler.alphas)\n",
    "plt.plot(ddim_noise_scheduler.alphas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robodiff",
   "language": "python",
   "name": "robodiff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
