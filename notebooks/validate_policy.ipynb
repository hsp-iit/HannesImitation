{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate trained policy offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../hannes-imitation')\n",
    "sys.path.append('../../hannes-imitation/hannes_imitation/external/diffusion_policy') # NOTE otherwise importing SequenceSampler fails\n",
    "\n",
    "# diffusion_policy imports\n",
    "from hannes_imitation.external.diffusion_policy.diffusion_policy.policy.diffusion_unet_image_policy import DiffusionUnetImagePolicy\n",
    "from hannes_imitation.external.diffusion_policy.diffusion_policy.common.pytorch_util import dict_apply\n",
    "\n",
    "# hannes_imitation imports\n",
    "from hannes_imitation.dataset.hannes_dataset import HannesImageDataset\n",
    "from hannes_imitation.dataset.hannes_dataset_hand_wrist_FE import HannesImageDatasetWrist\n",
    "\n",
    "from hannes_imitation.common import plot_utils\n",
    "from hannes_imitation.common.data_utils import resize_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "#merged_dir = '/home/calessi-iit.local/Projects/hannes-imitation/data/preliminary/'\n",
    "merged_dir = '/home/calessi-iit.local/Projects/hannes-imitation/data/mustard_hand_wrist_FE/'\n",
    "merged_name = 'merged.zarr'\n",
    "zarr_path = os.path.join(merged_dir, merged_name)\n",
    "#keys = ['image_in_hand', 'ref_move_hand']\n",
    "keys = ['image_in_hand', 'ref_move_hand', 'ref_move_wrist_FE']\n",
    "\n",
    "val_ratio = 0.1\n",
    "seed = 72\n",
    "max_train_episodes = None\n",
    "horizon = 16 # prediction horizon\n",
    "observation_horizon = 2\n",
    "action_horizon = 8\n",
    "pad_before = observation_horizon - 1\n",
    "pad_after = action_horizon - 1\n",
    "\n",
    "# training and validation dataset\n",
    "#train_dataset = HannesImageDataset(zarr_path, keys, horizon=horizon, pad_before=pad_before, pad_after=pad_after, seed=seed, val_ratio=val_ratio, max_train_episodes=None)\n",
    "train_dataset = HannesImageDatasetWrist(zarr_path, keys, horizon=horizon, pad_before=pad_before, pad_after=pad_after, seed=seed, val_ratio=val_ratio, max_train_episodes=None)\n",
    "validation_dataset = train_dataset.get_validation_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "#policy_path = '/home/calessi-iit.local/Projects/hannes-imitation/trainings/preliminary_policy.pth' # hand\n",
    "policy_path = '/home/calessi-iit.local/Projects/hannes-imitation/trainings/preliminary_policy_wrist_FE.pth' # hand, wrist_FE\n",
    "policy_path = '/home/calessi-iit.local/Projects/hannes-imitation/trainings/preliminary_policy_wrist_FE-tmp.pth' # hand, wrist_FE\n",
    "\n",
    "checkpoint = torch.load(policy_path)\n",
    "\n",
    "policy = checkpoint['policy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device transfer\n",
    "device = torch.device('cuda')\n",
    "_ = policy.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose episode\n",
    "episode = train_dataset.replay_buffer.get_episode(2)\n",
    "#episode = validation_dataset.replay_buffer.get_episode(10)\n",
    "\n",
    "frames = episode['image_in_hand']\n",
    "#actions_gt = episode['ref_move_hand']\n",
    "actions_gt = np.concatenate((episode['ref_move_hand'], episode['ref_move_wrist_FE']), axis=1)\n",
    "\n",
    "episode_len, height, width, _ = frames.shape\n",
    "\n",
    "sampling_frequency = 20 # Hz NOTE we decided this based on hand frequency and camera frequency\n",
    "\n",
    "frames.shape, actions_gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate policy on episode\n",
    "action_trajectories = []\n",
    "\n",
    "# NOTE: copy this loop also for inference on real robot\n",
    "frames_deque = collections.deque(iterable=[], maxlen=observation_horizon) # (B=1, To, C, H, W)\n",
    "\n",
    "for i in range(episode_len):\n",
    "    # read image, preprocess image, insert image in deque\n",
    "    image = frames[i] # read_image\n",
    "    image = np.moveaxis(image, source=-1, destination=0) # move Channel dimension first\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "\n",
    "    # append image to deque\n",
    "    # at the beginning the frames deque is empty. If it is the first iteration, fill the deque with copies of the first frame\n",
    "    frames_deque.append(image)    \n",
    "    while len(frames_deque) < frames_deque.maxlen:\n",
    "        frames_deque.append(image)\n",
    "\n",
    "    # create observation dictionary \n",
    "    obs_dict = {'image_in_hand': np.expand_dims(frames_deque, axis=0)} # include batch size in first dimension\n",
    "    obs_dict = dict_apply(obs_dict, torch.from_numpy)\n",
    "\n",
    "    # predict action trajectory\n",
    "    action_predictions = policy.predict_action(obs_dict) # {'action', 'action_pred}\n",
    "    action_trajectory = action_predictions['action'] # (B, Ta, Da)\n",
    "\n",
    "    action_trajectories.append(action_trajectory.cpu().detach().numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_trajectories = np.array(action_trajectories).reshape(episode_len, action_horizon, -1) # remove batch dimension (episode len, Ta, Da)\n",
    "#action_trajectories = np.round(action_trajectories) # convert to integers [0-100] ?\n",
    "action_trajectories.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action error over time\n",
    "\n",
    "```python\n",
    "action_error_0 = np.mean(np.abs(actions_gt - action_trajectories[:, 0]), axis=1) # no horizon\n",
    "action_error_1 = np.mean(np.abs(actions_gt[1:] - action_trajectories[:-1, 1]), axis=1) # horizon 1\n",
    "action_error_2 = np.mean(np.abs(actions_gt[2:] - action_trajectories[:-2, 2]), axis=1) # horizon 2\n",
    "```\n",
    "\n",
    "Idea illustrated below (sample time goes right, prediction horizon goes down): [x means that that index is ignored]\n",
    "\n",
    "for horizon 0 (i.e. 1 step prediction)\n",
    "- actions_gt      [a0, a1, a2, a3, ..., aN]\n",
    "- actions_pred_h0 [a00, a10, a20, a30, ... aN0] \n",
    "\n",
    "for horizon 1 (i.e., 2 step prediciton)\n",
    "- actions_gt      [x, a1, a2, a3, ..., a_N]\n",
    "- actions_pred_h1 [a01, a11, a21, ..., a_(N-1,1), x] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_errors_horizon = {h: None for h in range(action_horizon)}\n",
    "for h in range(action_horizon):\n",
    "    a_gt_horizon = actions_gt[h:] # skip the first h actions to get ground truth with horizon h\n",
    "    if h > 0:\n",
    "        a_pred_horizon = action_trajectories[:-h, h] # skip last h samples but take the prediction with horizon h starting from the first sample\n",
    "    else:\n",
    "        a_pred_horizon = action_trajectories[:, h] # NOTE: this because [:-0] does not work as expected\n",
    "\n",
    "    # average over action dimension\n",
    "    action_errors_horizon[h] = np.mean(np.abs(a_gt_horizon - a_pred_horizon), axis=1) # (episode_len, 1)\n",
    "    #action_errors_horizon[h] = np.abs(a_gt_horizon - a_pred_horizon) # (episode_len, 1)\n",
    "\n",
    "alphas = np.ones(action_horizon)\n",
    "alphas[1:] = np.linspace(0.5, 0.1, num=action_horizon-1)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(action_errors_horizon[h], color='tab:red', linewidth=2, alpha=alphas[0], label='0')\n",
    "\n",
    "for h in range(1, action_horizon):\n",
    "    plt.plot(action_errors_horizon[h], color='grey', alpha=alphas[h], label='%d' % h)\n",
    "\n",
    "plt.xlabel(\"Control step\")\n",
    "plt.ylabel(\"Action error\")\n",
    "\n",
    "plt.grid(linewidth=0.5, linestyle='--')\n",
    "plt.legend(title='Horizon, $T_a$', loc='best', ncols=2)\n",
    "plt.xlim([50, 100]) # break axis?\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(action_horizon):\n",
    "    print(\"horizon %d, MAE: %.2f +- %.2f\" % (i, action_errors_horizon[i].mean(), action_errors_horizon[i].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference vs predicted actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE ACTION\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(actions_gt, color='red', linestyle='--', linewidth=2, alpha=0.8, label='Reference')\n",
    "plt.plot(np.arange(0, episode_len), action_trajectories[:, 0], color='blue', linewidth=2, alpha=0.8, label='Prediction, $0$')\n",
    "\n",
    "for h in range(1, action_horizon):\n",
    "    time_axis = np.arange(h, episode_len)\n",
    "    plt.plot(time_axis, action_trajectories[:-h, h], color='grey', alpha=alphas[h], label='%d' % h)\n",
    "\n",
    "plt.xlabel(\"Control step\")\n",
    "plt.ylabel(\"Action (move_hand)\")\n",
    "\n",
    "plt.grid(linewidth=0.5, linestyle='--')\n",
    "#plt.xlim([50, 100]) # break axis?\n",
    "\n",
    "plt.legend(loc='best', ncols=2)\n",
    "#plt.savefig(fname='../figures/learning_curve.pdf', bbox_inches='tight', dpi=600)\n",
    "#plt.savefig(fname='../figures/learning_curve.png', bbox_inches='tight', dpi=600)\n",
    "#plt.savefig(fname='../figures/learning_curve.svg', bbox_inches='tight', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TWO ACTIONS\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(6,4), sharex=True)\n",
    "\n",
    "axes[0].plot(actions_gt[:, 0], color='red', linestyle='--', linewidth=2, alpha=0.8, label='Reference')\n",
    "axes[0].plot(np.arange(episode_len), action_trajectories[:, 0, 0], color='blue', linewidth=2, alpha=0.8, label='Prediction, $h=0$')\n",
    "axes[0].set_ylabel(\"Hand\")\n",
    "axes[0].grid(linewidth=0.5, linestyle='--')\n",
    "axes[0].legend(loc='best', ncols=1)\n",
    "\n",
    "axes[1].plot(actions_gt[:, 1], color='red', linestyle='--', linewidth=2, alpha=0.8)\n",
    "axes[1].plot(np.arange(episode_len), action_trajectories[:, 0, 1], color='blue', linewidth=2, alpha=0.8)\n",
    "axes[1].set_ylabel(\"Wrist F/E\")\n",
    "axes[1].grid(linewidth=0.5, linestyle='--')\n",
    "\n",
    "#for h in range(1, action_horizon):\n",
    "#    time_axis = np.arange(h, episode_len)\n",
    "#    plt.plot(time_axis, action_trajectories[:-h, h, 0], color='grey', alpha=alphas[h], label='%d' % h)\n",
    "\n",
    "plt.xlabel(\"Control step\")\n",
    "#plt.xlim([50, 100]) # break axis?\n",
    "\n",
    "#plt.savefig(fname='../figures/learning_curve.pdf', bbox_inches='tight', dpi=600)\n",
    "#plt.savefig(fname='../figures/learning_curve.png', bbox_inches='tight', dpi=600)\n",
    "#plt.savefig(fname='../figures/learning_curve.svg', bbox_inches='tight', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video (frames + predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE ACTION\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, nrows=1, figsize=(10, 5))\n",
    "\n",
    "# images\n",
    "line_img = ax1.imshow(resize_image(frames[0], 2))\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "\n",
    "# actions\n",
    "line_action_gt = ax2.plot(np.arange(0, episode_len), actions_gt, color='red', linestyle='--', linewidth=2, alpha=0.8, label='Reference')[0]\n",
    "line_action_pred = ax2.plot(np.arange(0, episode_len), action_trajectories[:, 0], color='blue', linewidth=2, alpha=0.8, label='Prediction')[0]\n",
    "\n",
    "ax2.set_xlabel(\"Control step\")\n",
    "ax2.set_ylabel(\"Action (move_hand)\")\n",
    "ax2.grid(linewidth=0.5, linestyle='--')\n",
    "ax2.legend(loc='upper left', ncols=1)\n",
    "\n",
    "# Initialize function\n",
    "def init():\n",
    "    line_img.set_data([[]])\n",
    "\n",
    "    line_action_gt.set_data([], [])\n",
    "    line_action_pred.set_data([], [])\n",
    "    \n",
    "    return line_img, line_action_gt, line_action_pred\n",
    "\n",
    "# Update function\n",
    "def update(frame):\n",
    "    line_img.set_data(frames[frame])\n",
    "\n",
    "    line_action_gt.set_data(np.arange(0, frame), actions_gt[:frame])\n",
    "    line_action_pred.set_data(np.arange(0, frame), action_trajectories[:frame, 0])\n",
    "\n",
    "    return line_img, line_action_gt, line_action_pred\n",
    "\n",
    "# Create animation\n",
    "ani = FuncAnimation(fig, update, frames=episode_len, init_func=init, blit=False)\n",
    "ani.save(\"../figures/hannes_policy_evaluation_offline.mp4\", writer=\"ffmpeg\", fps=sampling_frequency, dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TWO ACTIONS\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "gs = gridspec.GridSpec(nrows=2, ncols=2, width_ratios=[2, 1], height_ratios=[1, 1])\n",
    "ax1 = fig.add_subplot(gs[:, 0])  # Span all rows in the first column\n",
    "ax2 = fig.add_subplot(gs[0, 1])  # Top-right\n",
    "ax3 = fig.add_subplot(gs[1, 1], sharex=ax2)  # Bottom-right\n",
    "\n",
    "# Create the left plot (spanning both rows), images\n",
    "line_img = ax1.imshow(resize_image(frames[0], 2))\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "ax1.set_title('Eye-in-hand camera view', fontweight=\"bold\")\n",
    "\n",
    "# Create the top-right plot\n",
    "line_action_gt_1 = ax2.plot(np.arange(0, episode_len), actions_gt[:, 0], color='red', linestyle='--', linewidth=2, alpha=0.8, label='Reference')[0]\n",
    "line_action_pred_1 = ax2.plot(np.arange(0, episode_len), action_trajectories[:, 0, 0], color='blue', linewidth=2, alpha=0.8, label='Prediction')[0]\n",
    "ax2.set_ylabel(\"Hand\")\n",
    "ax2.grid(linewidth=0.5, linestyle='--')\n",
    "ax2.legend(loc='upper left', ncols=1)\n",
    "\n",
    "# Create the bottom-right plot\n",
    "line_action_gt_2 = ax3.plot(np.arange(0, episode_len), actions_gt[:, 1], color='red', linestyle='--', linewidth=2, alpha=0.8, label='Reference')[0]\n",
    "line_action_pred_2 = ax3.plot(np.arange(0, episode_len), action_trajectories[:, 0, 1], color='blue', linewidth=2, alpha=0.8, label='Prediction')[0]\n",
    "ax3.set_ylabel(\"Wrist F/E\")\n",
    "ax3.grid(linewidth=0.5, linestyle='--')\n",
    "ax3.set_xlabel(\"Control step\")\n",
    "\n",
    "# Adjust layout for clarity\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Initialize function\n",
    "def init():\n",
    "    line_img.set_data([[]])\n",
    "\n",
    "    line_action_gt_1.set_data([], [])\n",
    "    line_action_pred_1.set_data([], [])\n",
    "    \n",
    "    line_action_gt_2.set_data([], [])\n",
    "    line_action_pred_2.set_data([], [])\n",
    "\n",
    "    return line_img, line_action_gt_1, line_action_pred_1, line_action_gt_2, line_action_pred_2\n",
    "\n",
    "# Update function\n",
    "def update(frame):\n",
    "    line_img.set_data(frames[frame])\n",
    "\n",
    "    line_action_gt_1.set_data(np.arange(0, frame), actions_gt[:frame, 0])\n",
    "    line_action_pred_1.set_data(np.arange(0, frame), action_trajectories[:frame, 0, 0])\n",
    "\n",
    "    line_action_gt_2.set_data(np.arange(0, frame), actions_gt[:frame, 1])\n",
    "    line_action_pred_2.set_data(np.arange(0, frame), action_trajectories[:frame, 0, 1])\n",
    "\n",
    "    return line_img, line_action_gt_1, line_action_pred_1, line_action_gt_2, line_action_pred_2\n",
    "\n",
    "# Create animation\n",
    "ani = FuncAnimation(fig, update, frames=episode_len, init_func=init, blit=False)\n",
    "ani.save(\"../figures/hannes_policy_evaluation_hand_wrist_FE_offline.mp4\", writer=\"ffmpeg\", fps=sampling_frequency, dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra: Count parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function\n",
    "count_parameters = lambda net: np.sum([np.prod(params.shape) for params in net.parameters()])\n",
    "\n",
    "unet_count = count_parameters(policy.model)\n",
    "resnet_count = count_parameters(policy.obs_encoder)\n",
    "\n",
    "print(\"UNet parameters:\", unet_count)\n",
    "print(\"ResNet parameters:\", resnet_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: compute average prediction time\n",
    "- compute the average time to predict an action trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# evaluate policy on episode\n",
    "prediction_times = np.zeros(episode_len)\n",
    "\n",
    "frames_deque = collections.deque(iterable=[], maxlen=observation_horizon) # (B=1, To, C, H, W)\n",
    "\n",
    "for i in range(episode_len):\n",
    "    # read image, preprocess image, insert image in deque\n",
    "    image = frames[i] # read_image\n",
    "    image = np.moveaxis(image, source=-1, destination=0) # move Channel dimension first\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "\n",
    "    # append image to deque\n",
    "    # at the beginning the frames deque is empty. If it is the first iteration, fill the deque with copies of the first frame\n",
    "    frames_deque.append(image)    \n",
    "    while len(frames_deque) < frames_deque.maxlen:\n",
    "        frames_deque.append(image)\n",
    "\n",
    "    # create observation dictionary \n",
    "    obs_dict = {'image_in_hand': np.expand_dims(frames_deque, axis=0)} # include batch size in first dimension\n",
    "    obs_dict = dict_apply(obs_dict, torch.from_numpy)\n",
    "\n",
    "    # predict action trajectory\n",
    "    tic = time.time()\n",
    "    action_predictions = policy.predict_action(obs_dict) # {'action', 'action_pred}\n",
    "    #action_trajectory = action_predictions['action'] # (B, Ta, Da)\n",
    "    toc = time.time()\n",
    "    \n",
    "    # save prediction time\n",
    "    prediction_times[i] = toc - tic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Diffusion iterations:\", policy.num_inference_steps) # policy.noise_scheduler.num_train_timesteps)\n",
    "print(\"Avg prediction time: %.1f +- %.3f s\" % (np.mean(prediction_times), np.std(prediction_times)))\n",
    "print(\"Avg prediction frequency: %.1f Hz\" % (1 / np.mean(prediction_times)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffusion iterations: 50\n",
    "Avg prediction time: 0.2 +- 0.143 s\n",
    "Avg prediction frequency: 4.3 Hz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffusion iterations: 100\n",
    "Avg prediction time: 0.5 +- 0.026 s\n",
    "Avg prediction frequency: 1.9 Hz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robodiff",
   "language": "python",
   "name": "robodiff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
