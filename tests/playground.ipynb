{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/calessi-iit.local/Projects/hannes-imitation/tests',\n",
       " '/home/calessi-iit.local/miniforge3/envs/robodiff/lib/python39.zip',\n",
       " '/home/calessi-iit.local/miniforge3/envs/robodiff/lib/python3.9',\n",
       " '/home/calessi-iit.local/miniforge3/envs/robodiff/lib/python3.9/lib-dynload',\n",
       " '',\n",
       " '/home/calessi-iit.local/miniforge3/envs/robodiff/lib/python3.9/site-packages',\n",
       " '/home/calessi-iit.local/Projects/hannes-imitation',\n",
       " '/home/calessi-iit.local/Projects/hannes-imitation/hannes_imitation/external/diffusion_policy']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import zarr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch \n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('/home/calessi-iit.local/Projects/hannes-imitation')\n",
    "sys.path.append('/home/calessi-iit.local/Projects/hannes-imitation/hannes_imitation/external/diffusion_policy') # NOTE otherwise importing SequenceSampler fails\n",
    "\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hannes_imitation.dataset.hannes_dataset import HannesImageDataset\n",
    "\n",
    "merged_dir = '/home/calessi-iit.local/Projects/hannes-imitation/data/preliminary/'\n",
    "merged_name = 'merged.zarr'\n",
    "zarr_path = os.path.join(merged_dir, merged_name)\n",
    "keys = ['image_in_hand', 'ref_move_hand']\n",
    "val_ratio = 0.1\n",
    "seed = 72\n",
    "max_train_episodes = None\n",
    "horizon = 16 # prediction horizon\n",
    "observation_horizon = 2\n",
    "action_horizon = 8\n",
    "pad_before = observation_horizon - 1\n",
    "pad_after = action_horizon - 1\n",
    "\n",
    "# training and validation dataset\n",
    "train_dataset = HannesImageDataset(zarr_path, keys, horizon=horizon, pad_before=pad_before, pad_after=pad_after, seed=seed, val_ratio=val_ratio, max_train_episodes=None)\n",
    "validation_dataset = train_dataset.get_validation_dataset()\n",
    "\n",
    "# get normalizer\n",
    "normalizer = train_dataset.get_normalizer()\n",
    "\n",
    "\n",
    "# create dataloaders for training and validation\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "shuffle = True\n",
    "\n",
    "# pin_memory = True accelerates cpu-gpu transfer\n",
    "# persistent_workers = True does not kill worker process after each epoch\n",
    "tr_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=True, persistent_workers=True)\n",
    "vl_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/calessi-iit.local/miniforge3/envs/robodiff/lib/python3.9/site-packages/torch/cuda/__init__.py:83: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484806139/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch['obs']['image_hand'] torch.Size([64, 16, 3, 96, 128]) torch.float32\n",
      "batch['action'] torch.Size([64, 16, 1]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "# visualize data in batch\n",
    "# TODO discard unused observations earlier\n",
    "batch = next(iter(tr_dataloader))\n",
    "print(\"batch['obs']['image_hand']\", batch['obs']['image_in_hand'].shape, batch['obs']['image_in_hand'].dtype)\n",
    "print(\"batch['action']\", batch['action'].shape, batch['action'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get image observation encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  obs_encoder:\n",
    "    _target_: diffusion_policy.model.vision.multi_image_obs_encoder.MultiImageObsEncoder\n",
    "    shape_meta: ${shape_meta}\n",
    "    rgb_model:\n",
    "      _target_: diffusion_policy.model.vision.model_getter.get_resnet\n",
    "      name: resnet18\n",
    "      weights: null\n",
    "    resize_shape: null\n",
    "    crop_shape: [76, 76]\n",
    "    # constant center crop\n",
    "    random_crop: True\n",
    "    use_group_norm: True\n",
    "    share_rgb_model: False\n",
    "    imagenet_norm: True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image_shape: &image_shape [3, 96, 96]\n",
    "shape_meta: &shape_meta\n",
    "  # acceptable types: rgb, low_dim\n",
    "  obs:\n",
    "    image:\n",
    "      shape: *image_shape\n",
    "      type: rgb\n",
    "    agent_pos:\n",
    "      shape: [2]\n",
    "      type: low_dim\n",
    "  action:\n",
    "    shape: [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hannes_imitation.external.diffusion_policy.diffusion_policy.model.vision.multi_image_obs_encoder import MultiImageObsEncoder\n",
    "from hannes_imitation.external.diffusion_policy.diffusion_policy.model.vision.model_getter import get_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create shape_meta\n",
    "item = train_dataset.__getitem__(0)\n",
    "_, C, H, W = item['obs']['image_in_hand'].shape\n",
    "_, action_dim = item['action'].shape\n",
    "\n",
    "# Make sure that `shape_meta` correspond to input and output shapes for your task.\n",
    "shape_meta = dict(obs=dict(), action=dict())\n",
    "shape_meta['obs']['image_in_hand'] = dict(shape=(C, H, W), type='rgb')\n",
    "shape_meta['action'] = dict(shape=[action_dim])\n",
    "\n",
    "# NOTE if you want to include more observations\n",
    "#shape_meta['obs']['image_in_head'] = dict(shape=(C, H, W), type='rgb')\n",
    "#shape_meta['obs']['hand_info'] = dict(shape=[6], type='low_dim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create observation encoder\n",
    "rgb_model = get_resnet('resnet18') # dict()\n",
    "\n",
    "# The MultiImageObsEncoder encodes image and low dimensional observations into a single observation.\n",
    "# The constructor requires 2 positional arguments (shape_meta, rgb_model).\n",
    "# rgb_model can be directly an nn.Module or a Dict[str,nn.Module]\n",
    "\n",
    "# Optionally, you can specify if the image is resized (`resize_shape`) and/or cropped (`crop_shape`, `random_crop`) and/or\n",
    "# normalized according to imagenet values (`imagenet_norm`)\n",
    "# These transformations are performed in the forward() method.\n",
    "# We only use imagenet_norm.\n",
    "observation_encoder = MultiImageObsEncoder(shape_meta=shape_meta, rgb_model=rgb_model,\n",
    "                                       resize_shape=None,\n",
    "                                       crop_shape=None,\n",
    "                                       random_crop=False,\n",
    "                                       use_group_norm=True,\n",
    "                                       share_rgb_model=True,\n",
    "                                       imagenet_norm=True)\n",
    "\n",
    "# freeze observation_encoder\n",
    "_ = observation_encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#item['obs']['image_in_head'] = torch.zeros_like(item['obs']['image_in_hand'])\n",
    "#item['obs']['hand_info'] = torch.ones((16,6))\n",
    "\n",
    "# The forward model takes as input the dictinonary of observation `obs_dict`\n",
    "# if obs_dict contains more observations of type `rgb`, their features are concatenated\n",
    "# if obs_dict contains observations with low dimensions (of type `low_dim`), they are directly concatenated after the images' features\n",
    "observation_encoder(item['obs']).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from hannes_imitation.model.diffusion.conditional_unet1d import ConditionalUnet1D\n",
    "#from hannes_imitation.model.vision.model_getter import get_resnet\n",
    "\n",
    "\n",
    "#from hannes_imitation.common.pytorch_util import replace_bn_with_gn\n",
    "\n",
    "# diffusion policy imports\n",
    "from hannes_imitation.external.diffusion_policy.diffusion_policy.model.diffusion.conditional_unet1d import ConditionalUnet1D\n",
    "#from hannes_imitation.external.diffusion_policy.diffusion_policy.model.vision.model_getter import get_resnet\n",
    "#from hannes_imitation.external.diffusion_policy.diffusion_policy.common.pytorch_util import replace_submodules\n",
    "\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct ResNet18 encoder\n",
    "# if you have multiple camera views, use seperate encoder weights for each view.\n",
    "vision_encoder = get_resnet('resnet18')\n",
    "\n",
    "# IMPORTANT!\n",
    "# replace all BatchNorm with GroupNorm to work with EMA\n",
    "# performance will tank if you forget to do this!\n",
    "vision_encoder = replace_bn_with_gn(vision_encoder)\n",
    "\n",
    "_ = vision_encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_feature_dim = observation_encoder.output_shape()[0] #512 #  ResNet18 output dimensionality\n",
    "#lowdim_obs_dim = 2 # # agent_pos is 2 dimensional\n",
    "\n",
    "action_dim = batch['action'].shape[-1]\n",
    "observation_dim = vision_feature_dim# + lowdim_obs_dim\n",
    "conditioning_dim = observation_dim * observation_horizon\n",
    "\n",
    "# create network object\n",
    "# input_dim, local_cond_dim=None, global_cond_dim=None, diffusion_step_embed_dim=256, down_dims=[256,512,1024], kernel_size=3, n_groups=8, cond_predict_scale=False)\n",
    "noise_predictor = ConditionalUnet1D(input_dim=action_dim, global_cond_dim=conditioning_dim, \n",
    "                                   diffusion_step_embed_dim=256,\n",
    "                                   down_dims=[32, 64, 128])#[256,512,1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the final arch has 2 parts\n",
    "nets = nn.ModuleDict({'observation_encoder': observation_encoder, \n",
    "                      'noise_predictor': noise_predictor})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device transfer\n",
    "#device = torch.device('cpu')\n",
    "device = torch.device('cuda')\n",
    "_ = nets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets['observation_encoder'].training, nets['noise_predictor'].training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo 1\n",
    "#with torch.no_grad():\n",
    "#    # example inputs\n",
    "#    image = torch.zeros((1, observation_horizon, 3, 96, 96))\n",
    "#    #agent_pos = torch.zeros((1, obs_horizon, 2))\n",
    "#    \n",
    "#    # vision encoder\n",
    "#    image = image.flatten(end_dim=1) # squeeze, (obs_horizon=2, 3, 96, 96)\n",
    "#    image_features = nets['vision_encoder'](image) # (obs_horizon=2, 512)\n",
    "#    image_features = torch.unsqueeze(image_features, dim=0) # unsqueeze, (1,2,512)\n",
    "#    \n",
    "#    #obs = torch.cat([image_features, agent_pos], dim=-1) # (1,2,514)\n",
    "#    obs = image_features#\n",
    "#\n",
    "#    # action diffusion\n",
    "#    noised_action = torch.randn((1, prediction_horizon, action_dim))\n",
    "#    diffusion_iter = torch.zeros((1,))\n",
    "#\n",
    "#    # the noise prediction network\n",
    "#    # takes noisy action, diffusion iteration and observation as input\n",
    "#    # predicts the noise added to action (1, pred_horizon=16, action_dim=2)\n",
    "#    # the conditional observation gets flattened into (1, 1028)\n",
    "#    noise = nets['noise_predictor'](\n",
    "#        sample=noised_action,\n",
    "#        timestep=diffusion_iter,\n",
    "#        global_cond=obs.flatten(start_dim=1)) #\n",
    "#\n",
    "#    # illustration of removing noise\n",
    "#    # the actual noise removal is performed by NoiseScheduler\n",
    "#    # and is dependent on the diffusion noise schedule\n",
    "#    denoised_action = noised_action - noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "\n",
    "# demo 2\n",
    "# for this demo, we use DDPMScheduler with 100 diffusion iterations\n",
    "# NOTE: the choice of beta schedule has big impact on performance. We found squared cosine works the best\n",
    "num_diffusion_iters = 100\n",
    "\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=num_diffusion_iters,\n",
    "                                beta_schedule='squaredcos_cap_v2',\n",
    "                                clip_sample=True, # clip output to [-1,1] to improve stability\n",
    "                                prediction_type='epsilon') # the network predicts noise (instead of denoised action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from hannes_imitation.model.diffusion.ema_model import EMAModel\n",
    "\n",
    "# Exponential Moving Average accelerates training and improves stability\n",
    "# holds a copy of the model weights\n",
    "#ema = EMAModel(parameters=nets.parameters(), power=0.75)\n",
    "#ema = EMAModel(model=nets, power=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resnet_normalizer = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#_ = normalizer.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hannes_imitation.external.diffusion_policy.diffusion_policy.policy.diffusion_unet_image_policy import DiffusionUnetImagePolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 21\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# DiffusionUnetImagePolicy requires 6 positional arguments\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# shape_meta is a dictionary that contains the shapes of observations and actions for the task\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# noise_scheduler is an instance of DDPMScheduler noise scheduler\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# NOTE: there are other parameters that we do not change, except for the UNet model size\u001b[39;00m\n\u001b[1;32m     12\u001b[0m policy \u001b[38;5;241m=\u001b[39m DiffusionUnetImagePolicy(shape_meta\u001b[38;5;241m=\u001b[39mshape_meta, \n\u001b[1;32m     13\u001b[0m                                   noise_scheduler\u001b[38;5;241m=\u001b[39mnoise_scheduler, \n\u001b[1;32m     14\u001b[0m                                   obs_encoder\u001b[38;5;241m=\u001b[39mobservation_encoder,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m                                   diffusion_step_embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\u001b[38;5;66;03m#256,\u001b[39;00m\n\u001b[1;32m     19\u001b[0m                                   down_dims\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m])\u001b[38;5;66;03m#(256,512,1024))\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/robodiff/lib/python3.9/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 927\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/robodiff/lib/python3.9/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/robodiff/lib/python3.9/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 579 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/robodiff/lib/python3.9/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/robodiff/lib/python3.9/site-packages/torch/nn/modules/module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 602\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniforge3/envs/robodiff/lib/python3.9/site-packages/torch/nn/modules/module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/robodiff/lib/python3.9/site-packages/torch/cuda/__init__.py:217\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# are found or any other error occurs\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    221\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero."
     ]
    }
   ],
   "source": [
    "# DiffusionUnetImagePolicy requires 6 positional arguments\n",
    "# shape_meta is a dictionary that contains the shapes of observations and actions for the task\n",
    "# noise_scheduler is an instance of DDPMScheduler noise scheduler\n",
    "# obs_encoder is an instance of MultiImageObsEncoder which encodes images and low dimensional observations as conditioning\n",
    "# horizon is the prediction horizon (action prediction horizon)\n",
    "# n_action_steps is the action execution horizon (how many actions are actually executed from the prediction)\n",
    "# n_obs_steps is the observation horizon (how many recent observations to include as condition)\n",
    "\n",
    "# NOTE: there are other parameters that we do not change, except for the UNet model size\n",
    "\n",
    "\n",
    "policy = DiffusionUnetImagePolicy(shape_meta=shape_meta, \n",
    "                                  noise_scheduler=noise_scheduler, \n",
    "                                  obs_encoder=observation_encoder,\n",
    "                                  horizon=horizon,\n",
    "                                  n_action_steps=action_horizon,\n",
    "                                  n_obs_steps=observation_horizon,\n",
    "                                  diffusion_step_embed_dim=128,#256,\n",
    "                                  down_dims=[32, 64, 128])#(256,512,1024))\n",
    "\n",
    "_ = policy.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = normalizer.to(device)\n",
    "\n",
    "policy.set_normalizer(normalizer=normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer and learning rate scheduler\n",
    "# Standard ADAM optimizer (NOTE that EMA parametesr are not optimized)\n",
    "optimizer = torch.optim.AdamW(params=policy.parameters(), lr=1e-4, weight_decay=1e-6)\n",
    "\n",
    "from hannes_imitation.model.common.lr_scheduler import get_scheduler\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from hannes_imitation.external.diffusion_policy.diffusion_policy.common.pytorch_util import dict_apply\n",
    "\n",
    "num_epochs = 100 #100 # 100\n",
    "\n",
    "# Cosine LR schedule with linear warmup\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='cosine',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=len(tr_dataloader) * num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hannes_imitation.trainer.trainer_diffusion_policy import TrainerDiffusionPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(policy, vl_dataloader, device):\n",
    "    vl_batch_losses = []\n",
    "\n",
    "    vl_dataloader_iterator = tqdm(iterable=vl_dataloader, desc='Validation', leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in vl_dataloader_iterator:\n",
    "            batch = dict_apply(batch, lambda x: x.to(device, non_blocking=True))\n",
    "\n",
    "            # compute average loss in minibatch\n",
    "            loss = policy.compute_loss(batch)\n",
    "\n",
    "            vl_batch_losses.append(loss.item())\n",
    "    \n",
    "    mean_vl_loss = np.mean(vl_batch_losses)\n",
    "\n",
    "    return mean_vl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_action_error(policy, dataloader, device):\n",
    "    # sample trajectory from training set, and evaluate difference\n",
    "    action_errors = list()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # device transfer\n",
    "            batch = dict_apply(batch, lambda x: x.to(device))\n",
    "\n",
    "            # extract observation and ground truth action (gt)\n",
    "            obs_dict = batch['obs']\n",
    "            gt_action = batch['action'] # (B, horizon, Da)\n",
    "            \n",
    "            # predict actions (results are in original scale)\n",
    "            result = policy.predict_action(obs_dict)\n",
    "            \n",
    "            pred_action = result['action_pred'] # (B, horizon, Da)\n",
    "\n",
    "            # compute action error (mean absolute error)\n",
    "            mse = torch.nn.functional.l1_loss(pred_action, gt_action)\n",
    "            action_errors.append(mse.item())\n",
    "    \n",
    "    mean_action_error = np.mean(action_errors)\n",
    "\n",
    "    return mean_action_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_iterator = tqdm(iterable=range(num_epochs), desc=\"Epoch\")\n",
    "\n",
    "history = {'epoch': list(), \n",
    "           'tr_loss': list(), \n",
    "           'vl_loss': list(),\n",
    "           'vl_action_error': list()}\n",
    "\n",
    "for epoch in epoch_iterator:\n",
    "    \n",
    "    tr_loss_epoch = list()\n",
    "\n",
    "    for batch in tr_dataloader:\n",
    "        # device transfer\n",
    "        batch = dict_apply(batch, lambda x: x.to(device))\n",
    "\n",
    "        # compute average loss in minibatch\n",
    "        # NOTE unused observations are discarded within\n",
    "        loss = policy.compute_loss(batch)\n",
    "\n",
    "        # optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        lr_scheduler.step() # step lr scheduler every batch. This is different from standard pytorch behavior\n",
    "        \n",
    "        # save loss batch\n",
    "        tr_loss_epoch.append(loss.item())\n",
    "    \n",
    "    # End of epoch, validate on validation set\n",
    "    vl_loss = validate(policy, vl_dataloader, device)\n",
    "    vl_action_error = evaluate_action_error(policy, vl_dataloader, device)\n",
    "\n",
    "    # save training epoch results \n",
    "    history['epoch'].append(epoch + 1)\n",
    "    history['tr_loss'].append(np.mean(tr_loss_epoch))\n",
    "    history['vl_loss'].append(vl_loss)\n",
    "    history['vl_action_error'].append(vl_action_error)\n",
    "\n",
    "    # log training epoch results\n",
    "    postfix = dict(tr_loss=np.mean(tr_loss_epoch),\n",
    "                   vl_loss=vl_loss,\n",
    "                   vl_action_error=vl_action_error)\n",
    "    epoch_iterator.set_postfix(ordered_dict=postfix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "policy_dir = '/home/calessi-iit.local/Projects/hannes-imitation/trainings/'\n",
    "policy_name = 'preliminary_policy.pth'\n",
    "policy_path = os.path.join(policy_dir, policy_name)\n",
    "\n",
    "training_dict = {'policy': policy.to('cpu'),\n",
    "                 'policy_state_dict': policy.state_dict(),\n",
    "                 'optimizer': optimizer,\n",
    "                 'noise_scheduler': noise_scheduler,\n",
    "                 'history': history\n",
    "                }\n",
    "\n",
    "torch.save(training_dict, policy_path)\n",
    "\n",
    "print(\"Training saved in %s\" % str(policy_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "checkpoint = torch.load('/home/calessi-iit.local/Projects/hannes-imitation/trainings/preliminary_policy.pth')\n",
    "\n",
    "policy = checkpoint['policy']\n",
    "history = checkpoint['history']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(6,4), sharex=True)\n",
    "ax1.plot(history['epoch'], history['tr_loss'], label='Training set', color='blue')\n",
    "ax1.plot(history['epoch'], history['vl_loss'], label='Validation set', color='red')\n",
    "ax1.set_yscale('log')\n",
    "#ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.grid()\n",
    "ax1.legend(ncols=2)\n",
    "\n",
    "ax2.plot(history['epoch'], history['vl_action_error'], color='red')\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel('Action error')\n",
    "ax2.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerDiffusionPolicy:\n",
    "\n",
    "    def __init__(self, \n",
    "                 policy: DiffusionUnetImagePolicy,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 normalizer,\n",
    "                 tr_dataloader: torch.utils.data.DataLoader,\n",
    "                 vl_dataloader: torch.utils.data.DataLoader = None,\n",
    "                 learning_rate_scheduler: torch.optim.lr_scheduler._LRScheduler = None,\n",
    "                 ):\n",
    "        \n",
    "        self.policy = policy\n",
    "        self.optimizer = optimizer,\n",
    "        self.normalizer = normalizer,\n",
    "        self.tr_dataloader = tr_dataloader\n",
    "        self.vl_dataloader = vl_dataloader\n",
    "        self.learning_rate_scheduler = learning_rate_scheduler\n",
    "\n",
    "        # set policy normalizer\n",
    "        policy.set_normalizer(normalizer=self.normalizer)\n",
    "\n",
    "\n",
    "    def run(self, num_epochs, device):\n",
    "        # device transfer\n",
    "        _ = self.policy.to(device)\n",
    "        _ = self.policy.normalizer.to(device)\n",
    "\n",
    "        history = {'epoch': list(), 'tr_loss': list(), 'vl_loss': list(), 'vl_action_error': list()}\n",
    "\n",
    "        # Training loop\n",
    "        epoch_iterator = tqdm(iterable=range(num_epochs), desc=\"Epoch\")\n",
    "        for epoch in epoch_iterator:\n",
    "            # train for one epoch\n",
    "            tr_loss = self._train_epoch(device)\n",
    "            \n",
    "            # End of epoch, validate on validation set\n",
    "            vl_loss = validate(self.policy, self.vl_dataloader, device)\n",
    "            vl_action_error = evaluate_action_error(self.policy, self.vl_dataloader, device)\n",
    "\n",
    "            # save training epoch results \n",
    "            history['epoch'].append(epoch + 1)\n",
    "            history['tr_loss'].append(tr_loss)\n",
    "            history['vl_loss'].append(vl_loss)\n",
    "            history['vl_action_error'].append(vl_action_error)\n",
    "\n",
    "            # log training epoch results\n",
    "            postfix = dict(tr_loss=tr_loss, vl_loss=vl_loss, vl_action_error=vl_action_error)\n",
    "            epoch_iterator.set_postfix(ordered_dict=postfix)\n",
    "\n",
    "        return history\n",
    "\n",
    "    def _train_epoch(self, device):\n",
    "        # hold average loss for each mini-batch\n",
    "        tr_loss_epoch = np.zeros(len(self.tr_dataloader))\n",
    "\n",
    "        for i, batch in enumerate(self.tr_dataloader):\n",
    "            # device transfer\n",
    "            batch = dict_apply(batch, lambda x: x.to(device))\n",
    "\n",
    "            # compute average loss in minibatch\n",
    "            # NOTE unused observations are discarded within\n",
    "            loss = policy.compute_loss(batch)\n",
    "\n",
    "            # compute loss gradient of minibatch\n",
    "            loss.backward()\n",
    "\n",
    "            # optimize\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            if self.learning_rate_scheduler:\n",
    "                self.learning_rate_scheduler.step() # step lr scheduler every batch. This is different from standard pytorch behavior\n",
    "                \n",
    "            # save loss batch\n",
    "            tr_loss_epoch[i] = loss.item()\n",
    "        \n",
    "        # average loss over all minibatches\n",
    "        mean_tr_loss_epoch = np.mean(tr_loss_epoch)\n",
    "\n",
    "        return mean_tr_loss_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tr_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TrainerDiffusionPolicy(policy=policy, \n",
    "                                 optimizer=optimizer,\n",
    "                                 tr_dataloader=tr_dataloader, \n",
    "                                 vl_dataloader=vl_dataloader, \n",
    "                                 learning_rate_scheduler=lr_scheduler)\n",
    "\n",
    "trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(nets, vl_dataloader):\n",
    "    vl_batch_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for nbatch in vl_dataloader:\n",
    "            obs_dict = nbatch['obs']\n",
    "            \n",
    "            # discard unused observation\n",
    "            # flatten observation horizon and batch size into a single dimension (B*observation_horizon), keeping other dimensions\n",
    "            # send to device\n",
    "            obs_dict = dict_apply(obs_dict, lambda x: x[:,:observation_horizon,...])\n",
    "            obs_dict = dict_apply(obs_dict, lambda x: x.reshape(-1,*x.shape[2:])) # (B*observation_horizon, n_channels, H, W)\n",
    "            obs_dict = dict_apply(obs_dict, lambda x: x.to(device))\n",
    "\n",
    "            # normalize batch (discard unused observation)\n",
    "            #nimage = nbatch['obs']['image_in_hand'][:, :observation_horizon] # (B, observation_horizon, n_channels, H, W)\n",
    "            #nimage = nimage.to(device)\n",
    "            #nimage = resnet_normalizer(nimage)\n",
    "\n",
    "            naction = nbatch['action'].to(device)\n",
    "            naction = normalizer['action'].normalize(naction)\n",
    "            #B, _, n_channels, H, W = nimage.shape\n",
    "\n",
    "            B, _, _ = nbatch['action'].shape\n",
    "\n",
    "            # encoder vision features\n",
    "            #images = nimage.reshape(-1, n_channels, H, W) # (B*observation_horizon, n_channels, H, W)\n",
    "            #image_features = nets['vision_encoder'](images) # (B*observation_horizon, features_dim)\n",
    "            #image_features = image_features.reshape(B, observation_horizon, -1) # (B, observation_horizon, features_dim)\n",
    "\n",
    "            # conditional observation flattened. Concatenate vision feature and, if any, other low-dim obs\n",
    "            #obs_features = torch.cat([image_features, nagent_pos], dim=-1)\n",
    "            #obs_features = image_features\n",
    "            #obs_cond = obs_features.reshape(B, -1) # (B, obs_horizon * obs_dim)\n",
    "            obs_cond = observation_encoder(obs_dict).reshape(B, -1)\n",
    "\n",
    "            # sample noise to add to actions\n",
    "            noise = torch.randn(naction.shape, device=device)\n",
    "\n",
    "            # sample a diffusion iteration for each data point\n",
    "            timesteps = torch.randint(low=0, high=noise_scheduler.config.num_train_timesteps, size=(B,), device=device).long()\n",
    "\n",
    "            # Forward diffusion process. Add noise to the clean images according to the noise magnitude at each diffusion iteration\n",
    "            noisy_actions = noise_scheduler.add_noise(naction, noise, timesteps)\n",
    "\n",
    "            # predict the noise residual\n",
    "            # the noise prediction network takes noisy action, diffusion iteration and observation as input and predicts the noise added to naction (1, pred_horizon=16, action_dim=2)\n",
    "            noise_pred = nets['noise_predictor'](noisy_actions, timesteps, global_cond=obs_cond)\n",
    "\n",
    "            # L2 loss\n",
    "            loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "            vl_batch_losses.append(loss.item())\n",
    "    \n",
    "    vl_loss = np.mean(vl_batch_losses)\n",
    "\n",
    "    return vl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {'epoch': list(), \n",
    "           'tr_loss': list(), \n",
    "           'vl_loss': list()}\n",
    "\n",
    "with tqdm(range(num_epochs), desc='Epoch') as tglobal:\n",
    "    # epoch loop\n",
    "    for epoch_idx in tglobal:\n",
    "        epoch_loss = list()\n",
    "        # batch loop\n",
    "        with tqdm(tr_dataloader, desc='Batch', leave=False) as tepoch:\n",
    "            for nbatch in tepoch:\n",
    "                # normalize batch (discard unused observation)\n",
    "                #nimage = nbatch['obs']['image_in_hand'][:, :observation_horizon] # (B, observation_horizon, n_channels, H, W)\n",
    "                #nimage = nimage.to(device)\n",
    "                #nimage = resnet_normalizer(nimage)\n",
    "                \n",
    "                obs_dict = nbatch['obs']\n",
    "                \n",
    "                # discard unused observation\n",
    "                #for key in obs_dict.keys():\n",
    "                #    obs_dict[key] = obs_dict[key][:, :observation_horizon]\n",
    "                # discard unused observation\n",
    "                obs_dict = dict_apply(obs_dict, lambda x: x[:,:observation_horizon,...])\n",
    "                # flatten observation horizon and batch size into a single dimension (B*observation_horizon), keeping other dimensions\n",
    "                obs_dict = dict_apply(obs_dict, lambda x: x.reshape(-1,*x.shape[2:])) # (B*observation_horizon, n_channels, H, W)\n",
    "                # send to device\n",
    "                obs_dict = dict_apply(obs_dict, lambda x: x.to(device))\n",
    "\n",
    "                naction = nbatch['action'].to(device)\n",
    "                naction = normalizer['action'].normalize(naction)\n",
    "                #B, _, n_channels, H, W = nimage.shape\n",
    "                B, _, _ = nbatch['action'].shape\n",
    "\n",
    "                # encoder vision features\n",
    "                #images = nimage.reshape(-1, n_channels, H, W) # (B*observation_horizon, n_channels, H, W)\n",
    "                #image_features = nets['vision_encoder'](images) # (B*observation_horizon, features_dim)\n",
    "                #image_features = image_features.reshape(B, observation_horizon, -1) # (B, observation_horizon, features_dim)\n",
    "\n",
    "                # conditional observation flattened. Concatenate vision feature and, if any, other low-dim obs\n",
    "                #obs_features = torch.cat([image_features, nagent_pos], dim=-1)\n",
    "                #obs_features = image_features\n",
    "                #obs_cond = obs_features.reshape(B, -1) # (B, obs_horizon * obs_dim)\n",
    "\n",
    "                obs_cond = observation_encoder(obs_dict).reshape(B, -1)\n",
    "\n",
    "                # sample noise to add to actions\n",
    "                noise = torch.randn(naction.shape, device=device)\n",
    "\n",
    "                # sample a diffusion iteration for each data point\n",
    "                timesteps = torch.randint(low=0, high=noise_scheduler.config.num_train_timesteps, size=(B,), device=device).long()\n",
    "\n",
    "                # Forward diffusion process. Add noise to the clean images according to the noise magnitude at each diffusion iteration\n",
    "                noisy_actions = noise_scheduler.add_noise(naction, noise, timesteps)\n",
    "\n",
    "                # predict the noise residual\n",
    "                # the noise prediction network takes noisy action, diffusion iteration and observation as input and predicts the noise added to naction (1, pred_horizon=16, action_dim=2)\n",
    "                noise_pred = nets['noise_predictor'](noisy_actions, timesteps, global_cond=obs_cond)\n",
    "\n",
    "                # L2 loss\n",
    "                loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "                # optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                lr_scheduler.step() # step lr scheduler every batch. This is different from standard pytorch behavior\n",
    "\n",
    "                # update Exponential Moving Average of the model weights\n",
    "                #ema.step(nets.parameters())\n",
    "                #ema.step(nets)\n",
    "\n",
    "                # logging\n",
    "                loss_cpu = loss.item()\n",
    "                epoch_loss.append(loss_cpu)\n",
    "                tepoch.set_postfix(loss=loss_cpu)\n",
    "\n",
    "        # evaluate model on validation set after each epoch\n",
    "        vl_loss = validate(nets=nets.to(device), vl_dataloader=vl_dataloader)\n",
    "\n",
    "        tglobal.set_postfix(loss=np.mean(epoch_loss))\n",
    "\n",
    "        history['epoch'].append(epoch_idx + 1)\n",
    "        history['tr_loss'].append(np.mean(epoch_loss))\n",
    "        history['vl_loss'].append(vl_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robodiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
